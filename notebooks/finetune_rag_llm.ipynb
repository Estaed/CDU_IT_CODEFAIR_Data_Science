{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune Recommender LLM with LoRA/PEFT\n",
        "\n",
        "This notebook fine-tunes a small chat model to produce grounded, JSON-formatted recommendations using synthetic SFT data derived from our reviews + retrieval. Artifacts are saved to `models/rag_llm/` and can be used by the merged `finetune_rag_llm.ipynb` notebook.\n",
        "\n",
        "## 🚀 **Clean Workflow - Run Steps in Order:**\n",
        "\n",
        "1. **Step 0** - Setup & retrieval artifacts\n",
        "2. **Step 1** - Configuration + Convert training data to chat format\n",
        "3. **Step 2** - Build synthetic SFT dataset  \n",
        "4. **Step 3** - Tokenizer/model setup\n",
        "5. **Step 4** - **Fine-tune model** ⚠️ (6-8 hours)\n",
        "6. **Step 5** - Save final model\n",
        "7. **Step 6** - **Test the model** ✅\n",
        "\n",
        "## ⚠️ **Important Notes:**\n",
        "- **Step 4** fine-tunes the model to produce working JSON responses\n",
        "- **Step 6** tests the fine-tuned model\n",
        "- All training cells have been cleaned up for clarity\n",
        "- Model saves to `models/rag_llm/` directory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 0 — Ensure retrieval artifacts exist\n",
        "\n",
        "- Builds FAISS index and metadata if missing (uses the same pipeline as the merged notebook)\n",
        "- Required for synthesizing SFT data from retrieval context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index and metadata already exist. Skipping build.\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer # type: ignore\n",
        "\n",
        "try:\n",
        "    import faiss  # type: ignore\n",
        "except ImportError:\n",
        "    raise SystemExit(\"faiss is required. Install with `pip install faiss-cpu` on Windows.\")\n",
        "\n",
        "import pyarrow.parquet as pq\n",
        "from tqdm import tqdm\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "EMBED_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "EMBED_MODEL_DIR = snapshot_download(repo_id=EMBED_MODEL)\n",
        "embedder = SentenceTransformer(EMBED_MODEL_DIR)\n",
        "\n",
        "DATA_PATH = Path('data/processed/reviews_with_stars.csv')\n",
        "PROJECT_ROOT = Path.cwd() if (Path.cwd() / 'data').exists() else Path.cwd().parent\n",
        "INDEX_DIR = PROJECT_ROOT / 'models' / 'rag_llm' / 'step_0'\n",
        "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "INDEX_PATH = INDEX_DIR / 'reviews_all-MiniLM-L6-v2.index'\n",
        "METADATA_PATH = PROJECT_ROOT / 'data' / 'rag_llm' / 'processed' / 'review_metadata.parquet'\n",
        "METADATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "MANIFEST_PATH = INDEX_DIR / 'manifest.json'\n",
        "MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "embedder = SentenceTransformer(MODEL_NAME)\n",
        "embedding_dim = embedder.get_sentence_embedding_dimension()\n",
        "\n",
        "\n",
        "def build_index(force_rebuild: bool = False):\n",
        "    needs_build = force_rebuild or not (INDEX_PATH.exists() and METADATA_PATH.exists() and MANIFEST_PATH.exists())\n",
        "    if not needs_build:\n",
        "        print('Index and metadata already exist. Skipping build.')\n",
        "        return\n",
        "    # Locate project base (folder containing data/processed), then pick labeled file\n",
        "    def _find_base_dir(start: Path) -> Path:\n",
        "        if (start / 'data' / 'generate_stars' / 'processed').exists():\n",
        "            return start\n",
        "        for parent in start.parents:\n",
        "            if (parent / 'data' / 'generate_stars' / 'processed').exists():\n",
        "                return parent\n",
        "        return start\n",
        "\n",
        "    BASE_DIR = _find_base_dir(Path.cwd())\n",
        "    candidates = [\n",
        "        BASE_DIR / 'data' / 'generate_stars' / 'processed' / 'reviews_with_stars.csv',\n",
        "        BASE_DIR / 'data' / 'generate_stars' / 'processed' / 'reviews_with_stars_trained.csv',\n",
        "    ]\n",
        "    data_path = next((p for p in candidates if p.exists()), None)\n",
        "    assert data_path is not None, f\"Missing labeled data under {BASE_DIR / 'data' / 'generate_stars' / 'processed'}. Run generate_stars.ipynb first.\"\n",
        "    print('Using labeled data at:', data_path)\n",
        "\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "    # Ensure stars_float exists (prefer float ratings). Derive from integer 'stars' if needed\n",
        "    if 'stars_float' not in df.columns:\n",
        "        if 'stars' in df.columns:\n",
        "            df['stars_float'] = pd.to_numeric(df['stars'], errors='coerce').astype(float)\n",
        "        else:\n",
        "            raise ValueError(\"No 'stars_float' or 'stars' column in labeled data.\")\n",
        "\n",
        "    # Minimal schema check for remaining fields\n",
        "    req = ['source', 'place', 'comment']\n",
        "    missing = [c for c in req if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Labeled data missing columns: {missing}\")\n",
        "    df = df.dropna(subset=['comment']).copy()\n",
        "    df['comment'] = df['comment'].astype(str).str.strip()\n",
        "    df = df[df['comment'].str.len() > 0].reset_index(drop=True)\n",
        "\n",
        "    texts = df['comment'].tolist()\n",
        "    n = len(texts)\n",
        "    embeddings = np.empty((n, embedding_dim), dtype='float32')\n",
        "    for start in tqdm(range(0, n, BATCH_SIZE), total=(n + BATCH_SIZE - 1)//BATCH_SIZE, desc=\"Embedding\"):\n",
        "        end = min(start + BATCH_SIZE, n)\n",
        "        batch = texts[start:end]\n",
        "        emb = embedder.encode(batch, batch_size=64, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
        "        embeddings[start:end] = emb\n",
        "\n",
        "    index = faiss.IndexFlatIP(embedding_dim)\n",
        "    index.add(embeddings)\n",
        "    faiss.write_index(index, str(INDEX_PATH))\n",
        "\n",
        "    metadata = df[['source', 'place', 'comment', 'stars_float']].copy()\n",
        "    metadata.insert(0, 'row_id', np.arange(len(metadata), dtype=np.int64))\n",
        "    METADATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    metadata.to_parquet(METADATA_PATH, index=False)\n",
        "\n",
        "    manifest = {\n",
        "        'model': MODEL_NAME,\n",
        "        'embedding_dim': int(embedding_dim),\n",
        "        'index_type': 'IndexFlatIP',\n",
        "        'index_path': str(INDEX_PATH),\n",
        "        'metadata_path': str(METADATA_PATH),\n",
        "        'num_vectors': int(index.ntotal)\n",
        "    }\n",
        "    with open(MANIFEST_PATH, 'w', encoding='utf-8') as f:\n",
        "        json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
        "    print('Built index and metadata.')\n",
        "\n",
        "\n",
        "# Ensure ready\n",
        "build_index(force_rebuild=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1 — Setup & configuration\n",
        "\n",
        "- Choose a small instruction model (CPU-friendly)\n",
        "- Define output directory `models/rag_llm/`\n",
        "- Reuse allowed places + retrieval to synthesize SFT data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "try:\n",
        "    import faiss  # type: ignore\n",
        "except ImportError:\n",
        "    raise SystemExit(\"faiss is required. Install with `pip install faiss-cpu` on Windows.\")\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "# PEFT / LoRA\n",
        "try:\n",
        "    from peft import LoraConfig, get_peft_model, PeftModel\n",
        "except ImportError:\n",
        "    raise SystemExit(\"peft is required. Install with `pip install peft`.\")\n",
        "\n",
        "BASE_MODEL = 'Qwen/Qwen3-0.6B'  # Qwen 0.6B for 8GB VRAM\n",
        "OUTPUT_DIR = (Path.cwd() if (Path.cwd() / 'data').exists() else Path.cwd().parent) / 'models' / 'rag_llm'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PROJECT_ROOT = Path.cwd() if (Path.cwd() / 'data').exists() else Path.cwd().parent\n",
        "INDEX_DIR = PROJECT_ROOT / 'models' / 'rag_llm' / 'step_0'\n",
        "with open(INDEX_DIR / 'manifest.json', 'r', encoding='utf-8') as f:\n",
        "    manifest = json.load(f)\n",
        "faiss_index = faiss.read_index(manifest['index_path'])\n",
        "md_df = pq.read_table(manifest['metadata_path']).to_pandas()\n",
        "ALLOWED_PLACES = sorted(md_df['place'].dropna().unique().tolist())\n",
        "# Ensure stars_float is present in metadata\n",
        "aassert_col = 'stars_float'\n",
        "if aassert_col not in md_df.columns:\n",
        "    raise SystemExit(\"Expected 'stars_float' in metadata; re-run Step 0 with force_rebuild=True.\")\n",
        "\n",
        "# Embeddings for retrieval context\n",
        "EMBED_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "\n",
        "def retrieve(query: str, k: int = 8) -> pd.DataFrame:\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype('float32')\n",
        "    scores, idx = faiss_index.search(q_emb, k)\n",
        "    hits = []\n",
        "    for i, s in zip(idx[0], scores[0]):\n",
        "        if i == -1:\n",
        "            continue\n",
        "        row = md_df.iloc[int(i)].to_dict()\n",
        "        row['score'] = float(s)\n",
        "        # For convenience, expose a float alias for downstream\n",
        "        row['stars'] = float(row.get('stars_float', float('nan')))\n",
        "        hits.append(row)\n",
        "    return pd.DataFrame(hits)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2 — Build synthetic SFT dataset\n",
        "\n",
        "- Create input/output pairs using retrieval context\n",
        "- Inputs: system prompt + allowed places + user query + review context\n",
        "- Targets: JSON with `recommended_places`, `reasoning`, and `citations`\n",
        "- Saves `data/rag_llm/rag_sft.jsonl`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "SFT synthesis: 100%|██████████| 10000/10000 [01:26<00:00, 115.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote SFT dataset: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\data\\rag_llm\\processed\\rag_sft.jsonl | examples: 10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPT = (\n",
        "    \"You are a travel recommendation assistant for Australian destinations. \"\n",
        "    \"Recommend only places from Allowed Places. Ground answers in the context. \"\n",
        "    \"Respond with JSON: {recommended_places: [..], reasoning: str, citations: [{place, source, stars, snippet}]}\"\n",
        ")\n",
        "\n",
        "# Target size (adjustable)\n",
        "NUM_EXAMPLES_TARGET = 10000\n",
        "RNG_SEED = 42\n",
        "random.seed(RNG_SEED)\n",
        "\n",
        "# Query templates and facets\n",
        "activities = [\n",
        "    \"short hikes\", \"lookouts\", \"waterfalls\", \"swimming spots\", \"wildlife\",\n",
        "    \"cultural experiences\", \"sunset views\", \"sunrise views\", \"family-friendly walks\",\n",
        "    \"night sky views\", \"quiet camping\", \"scenic drives\"\n",
        "]\n",
        "modifiers = [\n",
        "    \"easy\", \"moderate\", \"kid-friendly\", \"photogenic\", \"less crowded\", \"near facilities\"\n",
        "]\n",
        "intents = []\n",
        "for _ in range(300):\n",
        "    a = random.choice(activities)\n",
        "    m = random.choice(modifiers)\n",
        "    intents.append(f\"{m} {a}\")\n",
        "\n",
        "# Ensure each allowed place appears by creating place-focused intents\n",
        "place_anchored = [f\"best things to do at {p}\" for p in ALLOWED_PLACES]\n",
        "USER_QUERIES = list(dict.fromkeys(intents + place_anchored))\n",
        "\n",
        "OUT_PATH = PROJECT_ROOT / 'data' / 'rag_llm' / 'processed' / 'rag_sft.jsonl'\n",
        "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def build_context(hits: pd.DataFrame, max_chars: int = 1200, max_rows: int = 8) -> str:\n",
        "    rows = []\n",
        "    used = 0\n",
        "    for _, r in hits.head(max_rows).iterrows():\n",
        "        snippet = str(r['comment'])\n",
        "        if len(snippet) > 240:\n",
        "            snippet = snippet[:240] + '...'\n",
        "        line = f\"- Place: {r['place']} | Source: {r['source']} | Stars: {float(r.get('stars', float('nan'))):.1f} | Review: {snippet}\"\n",
        "        if used + len(line) > max_chars:\n",
        "            break\n",
        "        rows.append(line)\n",
        "        used += len(line)\n",
        "    return \"\\n\".join(rows)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "num_written = 0\n",
        "max_rounds = 10000  # hard cap to avoid infinite loop if retrieval becomes empty\n",
        "with open(OUT_PATH, 'w', encoding='utf-8') as f:\n",
        "    pbar = tqdm(total=NUM_EXAMPLES_TARGET, desc='SFT synthesis')\n",
        "    rounds = 0\n",
        "    while num_written < NUM_EXAMPLES_TARGET and rounds < max_rounds:\n",
        "        # Shuffle intents each round for diversity\n",
        "        for q in random.sample(USER_QUERIES, len(USER_QUERIES)):\n",
        "            # Write up to N variants per intent until we hit the target\n",
        "            for _ in range(8):\n",
        "                if num_written >= NUM_EXAMPLES_TARGET:\n",
        "                    break\n",
        "                # Randomize retrieval by adding a small jitter and sampling top-k\n",
        "                k = 12\n",
        "                hits = retrieve(q, k=k)\n",
        "                if hits.empty:\n",
        "                    continue\n",
        "                # Shuffle to diversify citations/contexts\n",
        "                hits = hits.sample(frac=1.0, random_state=random.randint(0, 10_000)).reset_index(drop=True)\n",
        "                context = build_context(hits, max_rows=8)\n",
        "                if not context:\n",
        "                    continue\n",
        "                # Choose top places by mean stars (on the shuffled subset)\n",
        "                top_places = (\n",
        "                    hits.groupby('place')['stars']\n",
        "                        .mean()\n",
        "                        .sort_values(ascending=False)\n",
        "                        .head(3)\n",
        "                        .index.tolist()\n",
        "                )  # 'stars' is a float alias\n",
        "                # Build citations subset (best-scored after shuffle)\n",
        "                cits = []\n",
        "                for _, r in hits.head(5).iterrows():\n",
        "                    cits.append({\n",
        "                        'place': r['place'], 'source': r['source'], 'stars': round(float(r.get('stars', float('nan'))), 1),\n",
        "                        'snippet': str(r['comment'])[:220]\n",
        "                    })\n",
        "                prompt = (\n",
        "                    f\"[SYSTEM]\\n{SYSTEM_PROMPT}\\n\\n\"\n",
        "                    f\"[ALLOWED_PLACES]\\n{', '.join(ALLOWED_PLACES)}\\n\\n\"\n",
        "                    f\"[USER_QUERY]\\n{q}\\n\\n\"\n",
        "                    f\"[REVIEW_CONTEXT]\\n{context}\\n\\n\"\n",
        "                )\n",
        "                target = {\n",
        "                    'recommended_places': [p for p in top_places if p in ALLOWED_PLACES],\n",
        "                    'reasoning': 'Based on reviews and stars for relevance to the query.',\n",
        "                    'citations': cits,\n",
        "                }\n",
        "                f.write(json.dumps({'instruction': prompt, 'output': target}, ensure_ascii=False) + \"\\n\")\n",
        "                num_written += 1\n",
        "                pbar.update(1)\n",
        "                if num_written >= NUM_EXAMPLES_TARGET:\n",
        "                    break\n",
        "        rounds += 1\n",
        "    pbar.close()\n",
        "\n",
        "print('Wrote SFT dataset:', OUT_PATH, '| examples:', num_written)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3 — Convert training data to proper chat format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting data/rag_llm/processed/rag_sft.jsonl to chat format...\n",
            "Using absolute paths:\n",
            "  Input: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\data/rag_llm/processed/rag_sft.jsonl\n",
            "  Output: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\data/rag_llm/processed/rag_sft_chat.jsonl\n",
            "Converted 100 examples...\n",
            "Converted 200 examples...\n",
            "Converted 300 examples...\n",
            "Converted 400 examples...\n",
            "Converted 500 examples...\n",
            "Converted 600 examples...\n",
            "Converted 700 examples...\n",
            "Converted 800 examples...\n",
            "Converted 900 examples...\n",
            "Converted 1000 examples...\n",
            "Converted 1100 examples...\n",
            "Converted 1200 examples...\n",
            "Converted 1300 examples...\n",
            "Converted 1400 examples...\n",
            "Converted 1500 examples...\n",
            "Converted 1600 examples...\n",
            "Converted 1700 examples...\n",
            "Converted 1800 examples...\n",
            "Converted 1900 examples...\n",
            "Converted 2000 examples...\n",
            "Converted 2100 examples...\n",
            "Converted 2200 examples...\n",
            "Converted 2300 examples...\n",
            "Converted 2400 examples...\n",
            "Converted 2500 examples...\n",
            "Converted 2600 examples...\n",
            "Converted 2700 examples...\n",
            "Converted 2800 examples...\n",
            "Converted 2900 examples...\n",
            "Converted 3000 examples...\n",
            "Converted 3100 examples...\n",
            "Converted 3200 examples...\n",
            "Converted 3300 examples...\n",
            "Converted 3400 examples...\n",
            "Converted 3500 examples...\n",
            "Converted 3600 examples...\n",
            "Converted 3700 examples...\n",
            "Converted 3800 examples...\n",
            "Converted 3900 examples...\n",
            "Converted 4000 examples...\n",
            "Converted 4100 examples...\n",
            "Converted 4200 examples...\n",
            "Converted 4300 examples...\n",
            "Converted 4400 examples...\n",
            "Converted 4500 examples...\n",
            "Converted 4600 examples...\n",
            "Converted 4700 examples...\n",
            "Converted 4800 examples...\n",
            "Converted 4900 examples...\n",
            "Converted 5000 examples...\n",
            "Converted 5100 examples...\n",
            "Converted 5200 examples...\n",
            "Converted 5300 examples...\n",
            "Converted 5400 examples...\n",
            "Converted 5500 examples...\n",
            "Converted 5600 examples...\n",
            "Converted 5700 examples...\n",
            "Converted 5800 examples...\n",
            "Converted 5900 examples...\n",
            "Converted 6000 examples...\n",
            "Converted 6100 examples...\n",
            "Converted 6200 examples...\n",
            "Converted 6300 examples...\n",
            "Converted 6400 examples...\n",
            "Converted 6500 examples...\n",
            "Converted 6600 examples...\n",
            "Converted 6700 examples...\n",
            "Converted 6800 examples...\n",
            "Converted 6900 examples...\n",
            "Converted 7000 examples...\n",
            "Converted 7100 examples...\n",
            "Converted 7200 examples...\n",
            "Converted 7300 examples...\n",
            "Converted 7400 examples...\n",
            "Converted 7500 examples...\n",
            "Converted 7600 examples...\n",
            "Converted 7700 examples...\n",
            "Converted 7800 examples...\n",
            "Converted 7900 examples...\n",
            "Converted 8000 examples...\n",
            "Converted 8100 examples...\n",
            "Converted 8200 examples...\n",
            "Converted 8300 examples...\n",
            "Converted 8400 examples...\n",
            "Converted 8500 examples...\n",
            "Converted 8600 examples...\n",
            "Converted 8700 examples...\n",
            "Converted 8800 examples...\n",
            "Converted 8900 examples...\n",
            "Converted 9000 examples...\n",
            "Converted 9100 examples...\n",
            "Converted 9200 examples...\n",
            "Converted 9300 examples...\n",
            "Converted 9400 examples...\n",
            "Converted 9500 examples...\n",
            "Converted 9600 examples...\n",
            "Converted 9700 examples...\n",
            "Converted 9800 examples...\n",
            "Converted 9900 examples...\n",
            "Converted 10000 examples...\n",
            "✅ Conversion complete! Saved to c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\data/rag_llm/processed/rag_sft_chat.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def convert_to_chat_format(input_file, output_file):\n",
        "    \"\"\"Convert instruction/output format to messages format for proper fine-tuning\"\"\"\n",
        "    \n",
        "    print(f\"Converting {input_file} to chat format...\")\n",
        "    \n",
        "    # Use absolute paths to avoid working directory issues\n",
        "    if not os.path.isabs(input_file):\n",
        "        # Get the project root by looking for the data directory\n",
        "        current_dir = os.path.abspath('')\n",
        "        project_root = current_dir\n",
        "        \n",
        "        # Walk up directories until we find the data folder\n",
        "        while project_root != os.path.dirname(project_root):  # Stop at root\n",
        "            if os.path.exists(os.path.join(project_root, 'data')):\n",
        "                break\n",
        "            project_root = os.path.dirname(project_root)\n",
        "        \n",
        "        input_file = os.path.join(project_root, input_file)\n",
        "        output_file = os.path.join(project_root, output_file)\n",
        "    \n",
        "    print(f\"Using absolute paths:\")\n",
        "    print(f\"  Input: {input_file}\")\n",
        "    print(f\"  Output: {output_file}\")\n",
        "    \n",
        "    # Verify input file exists\n",
        "    if not os.path.exists(input_file):\n",
        "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
        "    \n",
        "    # Use UTF-8 encoding to handle Unicode characters\n",
        "    with open(input_file, 'r', encoding='utf-8') as f_in, open(output_file, 'w', encoding='utf-8') as f_out:\n",
        "        for line_num, line in enumerate(f_in):\n",
        "            try:\n",
        "                data = json.loads(line.strip())\n",
        "                \n",
        "                # Extract instruction and output\n",
        "                instruction = data['instruction']\n",
        "                output = data['output']\n",
        "                \n",
        "                # Convert to chat format\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a travel recommendation assistant for Australian destinations. Recommend only places from Allowed Places. Ground answers in the context. Respond with JSON: {recommended_places: [..], reasoning: str, citations: [{place, source, stars, snippet}]}\"},\n",
        "                    {\"role\": \"user\", \"content\": instruction},\n",
        "                    {\"role\": \"assistant\", \"content\": json.dumps(output)}\n",
        "                ]\n",
        "                \n",
        "                # Write new format\n",
        "                new_data = {\"messages\": messages}\n",
        "                f_out.write(json.dumps(new_data) + '\\n')\n",
        "                \n",
        "                if (line_num + 1) % 100 == 0:\n",
        "                    print(f\"Converted {line_num + 1} examples...\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Error processing line {line_num + 1}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    print(f\"✅ Conversion complete! Saved to {output_file}\")\n",
        "\n",
        "# Convert the training data\n",
        "convert_to_chat_format('data/rag_llm/processed/rag_sft.jsonl', 'data/rag_llm/processed/rag_sft_chat.jsonl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4 — Tokenizer, model, and LoRA config\n",
        "\n",
        "- Load base chat model + tokenizer\n",
        "- Attach LoRA adapters (low‑rank update on attention/projection layers)\n",
        "- Keep it CPU-friendly (no 8‑bit quantization required)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 10,092,544 || all params: 606,142,464 || trainable%: 1.6650\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "\n",
        "# Probe module names and select safe LoRA targets\n",
        "all_names = [n for n, _ in base_model.named_modules()]\n",
        "# common candidates across LLaMA-like + MobileLLM variants\n",
        "candidates = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "selected = [n.split(\".\")[-1] for n in all_names if any(c in n.split(\".\")[-1] for c in candidates)]\n",
        "# dedupe and keep only the layer names\n",
        "selected = sorted(list({s for s in selected if s in candidates}))\n",
        "if not selected:\n",
        "    selected = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=selected,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5 — Fine-tune model (DONT RUN THIS CELL IF YOU HAVE ALREADY FINAL FOLDER IN MODELS FOLDER)\n",
        "\n",
        "**What this does**:\n",
        "- Uses proper chat format data (rag_sft_chat.jsonl)\n",
        "- Proper training parameters for Qwen model\n",
        "- Saves to models/rag_llm/ directory\n",
        "- Will produce working JSON responses instead of garbled text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data...\n",
            "Using data file: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\data/rag_llm/processed/rag_sft_chat.jsonl\n",
            "Training samples: 9000\n",
            "Evaluation samples: 1000\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\TARIK\\AppData\\Local\\Temp\\ipykernel_21700\\1613519605.py:93: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13500' max='13500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13500/13500 12:19:52, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.044100</td>\n",
              "      <td>0.043223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.034200</td>\n",
              "      <td>0.034483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.031700</td>\n",
              "      <td>0.031618</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Fine-tuning complete!\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import os\n",
        "\n",
        "# Load the training data\n",
        "print(\"Loading training data...\")\n",
        "\n",
        "# Resolve path to avoid working directory issues\n",
        "data_file = 'data/rag_llm/processed/rag_sft_chat.jsonl'\n",
        "if not os.path.isabs(data_file):\n",
        "    # Get the project root by looking for the data directory\n",
        "    current_dir = os.path.abspath('')\n",
        "    project_root = current_dir\n",
        "    \n",
        "    # Walk up directories until we find the data folder\n",
        "    while project_root != os.path.dirname(project_root):  # Stop at root\n",
        "        if os.path.exists(os.path.join(project_root, 'data')):\n",
        "            break\n",
        "        project_root = os.path.dirname(project_root)\n",
        "    \n",
        "    data_file = os.path.join(project_root, data_file)\n",
        "\n",
        "print(f\"Using data file: {data_file}\")\n",
        "dataset = load_dataset('json', data_files=data_file)['train']\n",
        "\n",
        "# Split into train/eval\n",
        "train_size = int(0.9 * len(dataset))\n",
        "eval_size = len(dataset) - train_size\n",
        "train_dataset = dataset.select(range(train_size))\n",
        "eval_dataset = dataset.select(range(train_size, train_size + eval_size))\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
        "\n",
        "\n",
        "# Preprocess: tokenize chat messages into input_ids\n",
        "from transformers import default_data_collator\n",
        "MAX_LEN = 512  # was 1024 for speed\n",
        "\n",
        "# Enable TF32 on supported GPUs and move model to device\n",
        "import torch\n",
        "try:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "except Exception:\n",
        "    pass\n",
        "_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(_device)\n",
        "\n",
        "def preprocess(example):\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False,\n",
        "        enable_thinking=False,\n",
        "    )\n",
        "    enc = tokenizer(prompt, truncation=True, max_length=MAX_LEN)\n",
        "    enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
        "    return enc\n",
        "\n",
        "dataset_tok = dataset.map(preprocess, remove_columns=dataset.column_names)\n",
        "train_tok = dataset_tok.select(range(train_size))\n",
        "eval_tok = dataset_tok.select(range(train_size, train_size + eval_size))\n",
        "\n",
        "data_collator = default_data_collator\n",
        "\n",
        "# Training arguments with better settings\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,            # try 2–4 if VRAM allows\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=50,\n",
        "    learning_rate=1e-4,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"tensorboard\",\n",
        "    logging_dir=str(OUTPUT_DIR / \"tb\"),\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_num_workers=4,\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_prefetch_factor=2,\n",
        "    optim=\"adamw_torch_fused\" if torch.cuda.is_available() else \"adamw_torch\",\n",
        "    tf32=True,\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=eval_tok,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting fine-tuning...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model\n",
        "final_dir = OUTPUT_DIR / \"final\"\n",
        "trainer.save_model(str(final_dir))\n",
        "tokenizer.save_pretrained(str(final_dir))\n",
        "\n",
        "print(\"✅ Fine-tuning complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6 — Save final model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved adapters to c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\models\\rag_llm\\final\n",
            "Saved tokenizer to c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\models\\rag_llm\\final\n"
          ]
        }
      ],
      "source": [
        "# Step 5 — Save final model bundle (run this AFTER training)\n",
        "import os, torch\n",
        "final_dir = OUTPUT_DIR / 'final'\n",
        "final_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    from peft import PeftModel\n",
        "    merged = PeftModel.from_pretrained(base_model, OUTPUT_DIR / 'adapters')\n",
        "    merged = merged.merge_and_unload()\n",
        "    merged.save_pretrained(final_dir)\n",
        "    torch.save(merged.state_dict(), final_dir / 'model_state.pth')\n",
        "    print('Saved merged model to', final_dir)\n",
        "except Exception as e:\n",
        "    model.save_pretrained(final_dir)\n",
        "    torch.save(model.state_dict(), final_dir / 'adapters_state.pth')\n",
        "    print('Saved adapters to', final_dir)\n",
        "\n",
        "# Ensure tokenizer and configs are persisted with final\n",
        "try:\n",
        "    tokenizer.save_pretrained(final_dir)\n",
        "    print('Saved tokenizer to', final_dir)\n",
        "except Exception as e:\n",
        "    print('Tokenizer save failed:', e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7a — Create ZIP file for GitHub upload (DONT USE IF YOU GET FINAL FILE FROM GITHUB)\n",
        "\n",
        "This cell creates a compressed ZIP file of the entire final folder for easy GitHub upload.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Checking for final folder at: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\models\\rag_llm\\final\n",
            "📦 Creating ZIP file from final folder...\n",
            "  📄 Added: adapters_state.pth (2,425,006,175 bytes)\n",
            "  📄 Added: adapter_config.json (751 bytes)\n",
            "  📄 Added: adapter_model.safetensors (40,422,168 bytes)\n",
            "  📄 Added: added_tokens.json (735 bytes)\n",
            "  📄 Added: chat_template.jinja (4,256 bytes)\n",
            "  📄 Added: merges.txt (1,671,853 bytes)\n",
            "  📄 Added: README.md (5,089 bytes)\n",
            "  📄 Added: special_tokens_map.json (644 bytes)\n",
            "  📄 Added: tokenizer.json (11,422,752 bytes)\n",
            "  📄 Added: tokenizer_config.json (5,643 bytes)\n",
            "  📄 Added: training_args.bin (5,969 bytes)\n",
            "  📄 Added: vocab.json (2,776,833 bytes)\n",
            "✅ ZIP file created successfully!\n",
            "   Original size: 2,481,322,868 bytes\n",
            "   Compressed size: 1,199,407,262 bytes\n",
            "   Compression ratio: 51.7%\n",
            "🎉 ZIP file is ready for GitHub upload!\n"
          ]
        }
      ],
      "source": [
        "# Create ZIP file of the entire final folder for GitHub upload\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def create_zip_file():\n",
        "    \"\"\"Create a ZIP file of the entire final folder for GitHub upload.\"\"\"\n",
        "    # Go up one directory from notebooks to project root\n",
        "    project_root = os.path.dirname(os.getcwd())\n",
        "    final_dir = os.path.join(project_root, \"models\", \"rag_llm\", \"final\")\n",
        "    zip_path = os.path.join(project_root, \"models\", \"rag_llm\", \"final.zip\")\n",
        "    \n",
        "    print(f\"🔍 Checking for final folder at: {final_dir}\")\n",
        "    \n",
        "    if not os.path.exists(final_dir):\n",
        "        print(\"❌ Final folder not found. Please ensure models/rag_llm/final exists.\")\n",
        "        return False\n",
        "    \n",
        "    if os.path.exists(zip_path):\n",
        "        zip_size = os.path.getsize(zip_path)\n",
        "        print(f\"✅ ZIP file already exists ({zip_size:,} bytes), no need to create.\")\n",
        "        return True\n",
        "    \n",
        "    print(f\"📦 Creating ZIP file from final folder...\")\n",
        "    \n",
        "    try:\n",
        "        total_size = 0\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "            for root, dirs, files in os.walk(final_dir):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    # Skip the ZIP file itself if it exists\n",
        "                    if file_path == zip_path:\n",
        "                        continue\n",
        "                    arcname = os.path.relpath(file_path, final_dir)\n",
        "                    zf.write(file_path, arcname)\n",
        "                    file_size = os.path.getsize(file_path)\n",
        "                    total_size += file_size\n",
        "                    print(f\"  📄 Added: {arcname} ({file_size:,} bytes)\")\n",
        "        \n",
        "        zip_size = os.path.getsize(zip_path)\n",
        "        compression_ratio = (1 - zip_size / total_size) * 100\n",
        "        print(f\"✅ ZIP file created successfully!\")\n",
        "        print(f\"   Original size: {total_size:,} bytes\")\n",
        "        print(f\"   Compressed size: {zip_size:,} bytes\")\n",
        "        print(f\"   Compression ratio: {compression_ratio:.1f}%\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error creating ZIP file: {e}\")\n",
        "        return False\n",
        "\n",
        "# Create the ZIP file\n",
        "success = create_zip_file()\n",
        "if success:\n",
        "    print(\"🎉 ZIP file is ready for GitHub upload!\")\n",
        "else:\n",
        "    print(\"❌ ZIP file creation failed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7b — Extract model folder from ZIP (IF YOU WANNA USE THE FILE FIRST YOU NEED TO UNZIP THE FINAL FOLDER)\n",
        "\n",
        "This cell extracts the entire final folder from the ZIP when needed for loading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Checking for final folder at: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\models\\rag_llm\\final\n",
            "🔧 Final folder not found or incomplete, checking for ZIP file...\n",
            "📦 Found ZIP file (1,199,407,262 bytes), extracting...\n",
            "📁 Created final directory: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\models\\rag_llm\\final\n",
            "📁 Moving extracted files to final directory...\n",
            "  📄 Moved: adapters_state.pth\n",
            "  📄 Moved: adapter_config.json\n",
            "  📄 Moved: adapter_model.safetensors\n",
            "  📄 Moved: added_tokens.json\n",
            "  📄 Moved: chat_template.jinja\n",
            "  📄 Moved: merges.txt\n",
            "  📄 Moved: special_tokens_map.json\n",
            "  📄 Moved: tokenizer.json\n",
            "  📄 Moved: tokenizer_config.json\n",
            "  📄 Moved: vocab.json\n",
            "  📄 Moved: training_args.bin\n",
            "✅ Files moved successfully! Model file: 2,425,006,175 bytes\n",
            "🎉 Model folder is ready for loading!\n"
          ]
        }
      ],
      "source": [
        "# Create final directory and extract model from ZIP if needed\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def extract_model_folder():\n",
        "    \"\"\"Extract the entire final folder from ZIP if needed.\"\"\"\n",
        "    # Go up one directory from notebooks to project root\n",
        "    project_root = os.path.dirname(os.getcwd())\n",
        "    final_dir = os.path.join(project_root, \"models\", \"rag_llm\", \"final\")\n",
        "    zip_path = os.path.join(project_root, \"models\", \"rag_llm\", \"final.zip\")\n",
        "    \n",
        "    print(f\"🔍 Checking for final folder at: {final_dir}\")\n",
        "    \n",
        "    # Check if final folder exists and has the main model file\n",
        "    if os.path.exists(final_dir) and os.path.exists(os.path.join(final_dir, \"adapters_state.pth\")):\n",
        "        print(\"✅ Final folder already exists with model file, no extraction needed.\")\n",
        "        return True\n",
        "    \n",
        "    print(\"🔧 Final folder not found or incomplete, checking for ZIP file...\")\n",
        "    \n",
        "    if os.path.exists(zip_path):\n",
        "        zip_size = os.path.getsize(zip_path)\n",
        "        print(f\"📦 Found ZIP file ({zip_size:,} bytes), extracting...\")\n",
        "        try:\n",
        "            # Create the final directory if it doesn't exist\n",
        "            os.makedirs(final_dir, exist_ok=True)\n",
        "            print(f\"📁 Created final directory: {final_dir}\")\n",
        "            \n",
        "            with zipfile.ZipFile(zip_path, 'r') as archive:\n",
        "                # Extract all files to the rag_llm directory (ZIP contains final folder)\n",
        "                archive.extractall(os.path.join(project_root, \"models\", \"rag_llm\"))\n",
        "            \n",
        "            # Verify extraction - check if files were extracted to final directory\n",
        "            model_file_path = os.path.join(final_dir, \"adapters_state.pth\")\n",
        "            if os.path.exists(model_file_path):\n",
        "                extracted_size = os.path.getsize(model_file_path)\n",
        "                print(f\"✅ Folder extracted successfully! Model file: {extracted_size:,} bytes\")\n",
        "                return True\n",
        "            else:\n",
        "                # Check if files were extracted to rag_llm directory instead\n",
        "                rag_llm_model_path = os.path.join(project_root, \"models\", \"rag_llm\", \"adapters_state.pth\")\n",
        "                if os.path.exists(rag_llm_model_path):\n",
        "                    # Move files to final directory\n",
        "                    import shutil\n",
        "                    print(\"📁 Moving extracted files to final directory...\")\n",
        "                    # Only move model-related files, not evaluation files\n",
        "                    model_files = [\n",
        "                        'adapters_state.pth', 'adapter_config.json', 'adapter_model.safetensors',\n",
        "                        'added_tokens.json', 'chat_template.jinja', 'merges.txt', \n",
        "                        'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json',\n",
        "                        'vocab.json', 'training_args.bin'\n",
        "                    ]\n",
        "                    \n",
        "                    for file in model_files:\n",
        "                        src = os.path.join(project_root, \"models\", \"rag_llm\", file)\n",
        "                        if os.path.exists(src):\n",
        "                            dst = os.path.join(final_dir, file)\n",
        "                            shutil.move(src, dst)\n",
        "                            print(f\"  📄 Moved: {file}\")\n",
        "                    \n",
        "                    # Move comet evaluation files back to rag_llm directory\n",
        "                    comet_files = ['comet_evaluation.json', 'comet_evaluation_detailed.json']\n",
        "                    for file in comet_files:\n",
        "                        src = os.path.join(final_dir, file)\n",
        "                        if os.path.exists(src):\n",
        "                            dst = os.path.join(project_root, \"models\", \"rag_llm\", file)\n",
        "                            shutil.move(src, dst)\n",
        "                            print(f\"  📄 Moved back: {file}\")\n",
        "                    \n",
        "                    if os.path.exists(model_file_path):\n",
        "                        extracted_size = os.path.getsize(model_file_path)\n",
        "                        print(f\"✅ Files moved successfully! Model file: {extracted_size:,} bytes\")\n",
        "                        return True\n",
        "                \n",
        "                print(\"❌ Extraction completed but model file not found.\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error extracting ZIP file: {e}\")\n",
        "            return False\n",
        "    \n",
        "    print(\"❌ No ZIP file found. Please ensure the model files exist.\")\n",
        "    return False\n",
        "\n",
        "# Run the extraction\n",
        "success = extract_model_folder()\n",
        "if success:\n",
        "    print(\"🎉 Model folder is ready for loading!\")\n",
        "else:\n",
        "    print(\"❌ Model folder extraction failed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8 — Chat with model\n",
        "\n",
        "- Load the saved model from `models/rag_llm/final`\n",
        "- Run a sample query through retrieval + model and print JSON\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Testing corrected fine-tuned model:\n",
            "============================================================\n",
            "\n",
            "🗣️  Question 1: best waterfalls and swimming spots\n",
            "🤖 Model response:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5954aa3e55d49cf9d9e5719404bdd98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 RETRIEVED CONTEXT:\n",
            "- Place: Kakadu | Source: GoogleMaps | Stars: 4.5 | Review: Lovely, there are secrete swimming spots with waterfalls throughout.\n",
            "- Place: Nitmiluk (Katherine Gorge) | Source: GoogleMaps | Stars: 4.9 | Review: Great waterfalls and swimming holes. Natural beauty everywhere you look.\n",
            "- Place: Kakadu | Source: GoogleMaps | Stars: 4.6 | Review: Great place to visit after the wet season and the waterfalls have opened back up, not much to do in the wet at all\n",
            "- Place: Kakadu National Park – Gunlom Falls | Source: TripAdvisor | Stars: 4.9 | Review: FANTASTIC!! When you get there you are greeted with a fantastic almost circular gorge, waterfall and pristine crystal clear water. A perfect place for a swim. Bring a noodle or a float of some kind because it is just gre\n",
            "- Place: Kakadu | Source: GoogleMaps | Stars: 4.9 | Review: The best place to visit. Amazing nature Amazing water falls everywhere beautiful\n",
            "\n",
            "==================================================\n",
            "\n",
            "- **Nitmiluk (Katherine Gorge)**\n",
            "- **Kakadu**\n",
            "\n",
            "============================================================\n",
            "\n",
            "🗣️  Question 2: family-friendly walks and sunset views\n",
            "🤖 Model response:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3450ad74dd16485ca0dda5b4ca4e4094",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 RETRIEVED CONTEXT:\n",
            "- Place: Uluru-Kata Tjuta | Source: GoogleMaps | Stars: 4.7 | Review: Outstanding views especially at sunset.\n",
            "- Place: Devils Marbles (Karlu Karlu) | Source: TripAdvisor | Stars: 4.5 | Review: Both sunrise and sunset are beautiful, get your camera ready for some great photos. There are some walks as well.\n",
            "- Place: Uluru-Kata Tjuta | Source: TripAdvisor | Stars: 4.3 | Review: Absolutely breathtaking to see at sunset the colours are amazing but extremely busy at this time of year we did some of the shorter walks really good.\n",
            "- Place: Devils Marbles (Karlu Karlu) | Source: GoogleMaps | Stars: 4.5 | Review: The sunset's are a must do\n",
            "- Place: Uluru-Kata Tjuta | Source: TripAdvisor | Stars: 4.9 | Review: We got to walk around the wonderful sight all day and stay for a sunset. The colors and setting is just beautiful. It's amazing to see if change colors right before your eyes. You must experience this amazing place.\n",
            "\n",
            "==================================================\n",
            "\n",
            "- **West MacDonnell – Ormiston Gorge**  \n",
            "  **Mode of Travel:** Car/Train  \n",
            "  **Ways to Get Near:** Olgas Highway, MacDonnell Road  \n",
            "  **What To Do At Olgas Highway (Near Ormiston Gorge):** Walk, bike, roll – enjoy the view\n",
            "\n",
            "- **Tjoritja / West MacDonnell National Park**  \n",
            "  **Highlights:** Walks, Camp sites, Photography\n",
            "\n",
            "- **Uluru-Kata Tjuta**  \n",
            "  **Highlights:** Spectacular sunset views 🌟 Great place for family days!\n",
            "\n",
            "============================================================\n",
            "\n",
            "🗣️  Question 3: places that are not too hot\n",
            "🤖 Model response:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5783e40660554075bd15a2cb37ad8104",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 RETRIEVED CONTEXT:\n",
            "- Place: Tjoritja / West MacDonnell National Park | Source: GoogleMaps | Stars: 4.6 | Review: A wonderful place to visit especially when it's hot\n",
            "- Place: Devils Marbles (Karlu Karlu) | Source: GoogleMaps | Stars: 3.4 | Review: Amazing place but it's way too hot 🥵 …\n",
            "- Place: Kakadu | Source: GoogleMaps | Stars: 2.7 | Review: Many nice places. But.. Many places closed during the wet season. (April and no water anywhere by the way)\n",
            "- Place: Kakadu | Source: GoogleMaps | Stars: 4.9 | Review: Best place ever\n",
            "- Place: Alice Springs Desert Park | Source: Reddit/AskAnAustralian | Stars: 2.5 | Review: Anywhere but Alice Springs. Rocky has its issues and you will learn a new definition of heat, but at least it’s not Alice.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Based on the reviews, here are the places that are \"not too hot\":\n",
            "\n",
            "- **West MacDonnell – Ormiston Gorge** (Source: TripAdvisor) | Stars: 4.8 | Review: This is a must do. It is hot in there but not at all hot out there. You can swim in the pool. Do the walks. Beautiful scenery. Do the diving. You will love it.\n",
            "- **Tjoritja / West MacDonnell National Park** (Source: GoogleMaps) | Stars: 4.7 | Review: Hot spots for swimming and camping. A walk for a family of 3 with a view. Great place for looking around. Not the most popular but absolutely charming.\n",
            "\n",
            "============================================================\n",
            "\n",
            "🎯 Interactive mode - Ask your own questions!\n",
            "Type 'quit' to exit\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ace6d8a55ecb432ba2721f9f1c330e2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 RETRIEVED CONTEXT:\n",
            "- Place: Tjoritja / West MacDonnell National Park | Source: GoogleMaps | Stars: 4.6 | Review: A wonderful place to visit especially when it's hot\n",
            "- Place: Devils Marbles (Karlu Karlu) | Source: GoogleMaps | Stars: 3.4 | Review: Amazing place but it's way too hot 🥵 …\n",
            "- Place: Alice Springs Desert Park | Source: Reddit/travel | Stars: 4.4 | Review: It gets super hot in Alice Springs\n",
            "- Place: Kakadu | Source: GoogleMaps | Stars: 4.6 | Review: Great place to visit. Gets hot and humid.\n",
            "- Place: Uluru-Kata Tjuta | Source: Reddit/NatureIsFuckingLit | Stars: 4.4 | Review: I visited Ayers Rock once many years ago and it's still one of the hottest places I've been. I can't remember what time of year I was there but the temperatures were in the upper 50's (Celsius)\n",
            "\n",
            "==================================================\n",
            "\n",
            "🤖 Model response: Based on reviews and stars:\n",
            "\n",
            "- **Tjoritja / West MacDonnell National Park**: This place is both hot and cold.  \n",
            "- **Devils Marbles (Karlu Karlu)**: This place is quite hot.  \n",
            "- **West MacDonnell – Ormiston Gorge**: This place is quite cold.  \n",
            "\n",
            "Thus, the most cold place is **West MacDonnell – Ormiston Gorge**, and the most hot place is **Devils Marbles (Karlu Karlu)**.\n"
          ]
        }
      ],
      "source": [
        "### Step 6 — Test the model\n",
        "import torch\n",
        "import re\n",
        "\n",
        "def test_corrected_model():\n",
        "    \"\"\"Test the corrected fine-tuned model\"\"\"\n",
        "    \n",
        "    # Load model from the actual saved path\n",
        "    model_path = str(OUTPUT_DIR / \"final\")\n",
        "    ft_tok = AutoTokenizer.from_pretrained(model_path)\n",
        "    ft_model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
        "    ft_model = ft_model.to('cuda' if torch.cuda.is_available() else 'cpu').eval()\n",
        "    \n",
        "    def ask_corrected(query: str, k: int = 8, show_context: bool = False) -> str:\n",
        "        hits = retrieve(query, k=k)\n",
        "        context = []\n",
        "        for _, r in hits.head(5).iterrows():\n",
        "            snippet = str(r['comment'])[:220]\n",
        "            context.append(f\"- Place: {r['place']} | Source: {r['source']} | Stars: {float(r.get('stars', float('nan'))):.1f} | Review: {snippet}\")\n",
        "        \n",
        "        if show_context:\n",
        "            print(\"🔍 RETRIEVED CONTEXT:\")\n",
        "            print(\"\\n\".join(context))\n",
        "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "        \n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful travel assistant for Australian destinations. ONLY recommend places from the context provided. Base your recommendations strictly on the reviews and ratings given.\"},\n",
        "            {\"role\": \"user\", \"content\": (\n",
        "                f\"Here are some places I can recommend: {', '.join(ALLOWED_PLACES)}\\n\\n\"\n",
        "                f\"User query: {query}\\n\\n\"\n",
        "                f\"Here's what people have said about these places:\\n\" + \"\\n\".join(context)\n",
        "            )}\n",
        "        ]\n",
        "        \n",
        "        prompt_text = ft_tok.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=False  # Disable thinking for cleaner output\n",
        "        )\n",
        "        \n",
        "        device = next(ft_model.parameters()).device\n",
        "        inputs = ft_tok(prompt_text, return_tensors='pt').to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            out = ft_model.generate(\n",
        "                **inputs, \n",
        "                max_new_tokens=200,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=ft_tok.eos_token_id\n",
        "            )\n",
        "        \n",
        "        text = ft_tok.decode(out[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract only the assistant's response\n",
        "        if \"assistant\" in text:\n",
        "            text = text.split(\"assistant\")[-1].strip()\n",
        "        \n",
        "        # Remove thinking tags if present\n",
        "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    # Test with sample queries\n",
        "    test_queries = [\n",
        "        \"best waterfalls and swimming spots\",\n",
        "        \"family-friendly walks and sunset views\",\n",
        "        \"places that are not too hot\"\n",
        "    ]\n",
        "    \n",
        "    print(\"🤖 Testing corrected fine-tuned model:\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for i, query in enumerate(test_queries, 1):\n",
        "        print(f\"\\n🗣️  Question {i}: {query}\")\n",
        "        print(\"🤖 Model response:\")\n",
        "        try:\n",
        "            response = ask_corrected(query, show_context=True)\n",
        "            print(response)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "    \n",
        "    # Interactive mode\n",
        "    print(\"\\n🎯 Interactive mode - Ask your own questions!\")\n",
        "    print(\"Type 'quit' to exit\")\n",
        "    \n",
        "    while True:\n",
        "        user_query = input(\"\\n🗣️  Your question: \").strip()\n",
        "        if user_query.lower() in ['quit', 'exit', 'q']:\n",
        "            break\n",
        "        if user_query:\n",
        "            try:\n",
        "                response = ask_corrected(user_query, show_context=True)\n",
        "                print(f\"🤖 Model response: {response}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error: {e}\")\n",
        "    \n",
        "    return ask_corrected\n",
        "\n",
        "# Run the test\n",
        "ask_corrected = test_corrected_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 9 — COMET evaluation\n",
        "\n",
        "Simple evaluation using COMET to score model responses against reference answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading COMET model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\TARIK\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n",
            "c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\venv\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading final model...\n",
            "Generating model responses...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfcdd9c202d14b0dbd4f1955cf193f06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: best waterfalls and swimming spots\n",
            "Model: - **Nitmiluk (Katherine Gorge)**\n",
            "- **Kakadu National Park – Gunlom Falls**\n",
            "- **Kakadu**\n",
            "- **Uluru-K...\n",
            "Reference: Nitmiluk (Katherine Gorge) offers great waterfalls and swimming holes. Kakadu National Park has Gunl...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b46fb7d66a0407394025afa520ba47e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: family-friendly walks and sunset views\n",
            "Model: - **West MacDonnell – Ormiston Gorge**  \n",
            "  *Difficulty: Easy*  \n",
            "  *Viewing Conditions: Good*  \n",
            "  *Wa...\n",
            "Reference: Devils Marbles (Karlu Karlu) provides beautiful sunset views and family-friendly walks. West MacDonn...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39c3d1d5baae41dbb62bd44e527b689e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: places that are not too hot\n",
            "Model: Based on the reviews, here are the places that are \"not too hot\":\n",
            "\n",
            "- **West MacDonnell – Ormiston Go...\n",
            "Reference: Nitmiluk (Katherine Gorge) offers cooler temperatures and peaceful natural settings. West MacDonnell...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cbcfe7e439a44eda27137395d016930",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: cultural experiences and wildlife viewing\n",
            "Model: - **Tjoritja / West MacDonnell National Park** – Explore ancient ecological patterns, native culture...\n",
            "Reference: Kakadu National Park offers rich cultural experiences with Aboriginal rock art and diverse wildlife....\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a78780613e734b4fbe44814378383b33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: scenic drives and photography spots\n",
            "Model: Here are some places with relevant reviews:\n",
            "\n",
            "- **Tjoritja / West MacDonnell National Park** (Scenic ...\n",
            "Reference: West MacDonnell National Park offers spectacular scenic drives with excellent photography opportunit...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2187bf64b3304a049e086aea0029247c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: quiet camping and stargazing\n",
            "Model: - Nitmiluk (Katherine Gorge) | Source: GoogleMaps | Stars: 4.4 | Review: Camp ground is quiet & rela...\n",
            "Reference: West MacDonnell National Park offers quiet camping spots with excellent stargazing opportunities. De...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb3c907c88ce4a2ab419994ab9b5f101",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: easy hikes for beginners\n",
            "Model: Based on reviews and stars, here are suitable recommendations for you:\n",
            "\n",
            "- **Nitmiluk (Katherine Gorg...\n",
            "Reference: Nitmiluk (Katherine Gorge) offers easy walking trails suitable for beginners. West MacDonnell Nation...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4eaaada623db43cc97b0716619034ffc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: places with good facilities and amenities\n",
            "Model: - Kakadu\n",
            "- West MacDonnell – Ormiston Gorge\n",
            "- Alice Springs Desert Park\n",
            "- Kakadu Gunlom Falls...\n",
            "Reference: Kakadu National Park has well-developed facilities and visitor centers. Uluru-Kata Tjuta offers comp...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e033668f022406c91c552d0f5d25fd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: water activities and boat tours\n",
            "Model: Based on the reviews and stars for places you've listed, here are recommendations for water activiti...\n",
            "Reference: Nitmiluk (Katherine Gorge) offers excellent boat tours and water activities. Kakadu National Park pr...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "063a44aa5e2e46daabd3ddf48ea9032d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: places to visit during wet season\n",
            "Model: - **Kakadu** (Source: GoogleMaps) | Stars: 3.9 | Review: The wet season is during November to Februa...\n",
            "Reference: Kakadu National Park is particularly beautiful during the wet season with flowing waterfalls. Nitmil...\n",
            "--------------------------------------------------\n",
            "Running COMET evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "DETAILED COMET EVALUATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "📊 OVERALL STATISTICS:\n",
            "   Total Queries: 10\n",
            "   Average Score: 0.5644\n",
            "   Min Score: 0.4670\n",
            "   Max Score: 0.7214\n",
            "\n",
            "📈 SCORE DISTRIBUTION:\n",
            "   Excellent (≥0.8): 0 (0.0%)\n",
            "   Good (0.6-0.8):  4 (40.0%)\n",
            "   Fair (0.4-0.6):  6 (60.0%)\n",
            "   Poor (<0.4):     0 (0.0%)\n",
            "\n",
            "================================================================================\n",
            "INDIVIDUAL RESULTS\n",
            "================================================================================\n",
            "\n",
            "🟠 Query 1: best waterfalls and swimming spots\n",
            "   COMET Score: 0.5550\n",
            "   Model Response: - **Nitmiluk (Katherine Gorge)**\n",
            "- **Kakadu National Park – Gunlom Falls**\n",
            "- **Kakadu**\n",
            "- **Uluru-Kata Tjuta** (with swimming spots)...\n",
            "   Reference: Nitmiluk (Katherine Gorge) offers great waterfalls and swimming holes. Kakadu National Park has Gunlom Falls with refreshing pools perfect for swimming. West MacDonnell National Park has natural water...\n",
            "\n",
            "🟠 Query 2: family-friendly walks and sunset views\n",
            "   COMET Score: 0.5522\n",
            "   Model Response: - **West MacDonnell – Ormiston Gorge**  \n",
            "  *Difficulty: Easy*  \n",
            "  *Viewing Conditions: Good*  \n",
            "  *Ways to Adapt: Get used to long walks (about 3.3 hours each) of the gorge. Be prepared for(strictly ne...\n",
            "   Reference: Devils Marbles (Karlu Karlu) provides beautiful sunset views and family-friendly walks. West MacDonnell National Park offers scenic walks suitable for families with spectacular sunset views....\n",
            "\n",
            "🟠 Query 3: places that are not too hot\n",
            "   COMET Score: 0.4760\n",
            "   Model Response: Based on the reviews, here are the places that are \"not too hot\":\n",
            "\n",
            "- **West MacDonnell – Ormiston Gorge** (Source: TripAdvisor) | Stars: 4.8 | Review: This is a must see if in the top 5 destinations y...\n",
            "   Reference: Nitmiluk (Katherine Gorge) offers cooler temperatures and peaceful natural settings. West MacDonnell National Park provides pleasant conditions for outdoor activities....\n",
            "\n",
            "🟡 Query 4: cultural experiences and wildlife viewing\n",
            "   COMET Score: 0.6285\n",
            "   Model Response: - **Tjoritja / West MacDonnell National Park** – Explore ancient ecological patterns, native culture, and natural wonders such as sandboxes for kids, swimming spots, and scenic drives. Enjoy the exper...\n",
            "   Reference: Kakadu National Park offers rich cultural experiences with Aboriginal rock art and diverse wildlife. Uluru-Kata Tjuta provides cultural significance and unique wildlife viewing opportunities....\n",
            "\n",
            "🟡 Query 5: scenic drives and photography spots\n",
            "   COMET Score: 0.6240\n",
            "   Model Response: Here are some places with relevant reviews:\n",
            "\n",
            "- **Tjoritja / West MacDonnell National Park** (Scenic drives and photos): *Great car park facilities*  \n",
            "- **West MacDonnell – Ormiston Gorge** (Photograph...\n",
            "   Reference: West MacDonnell National Park offers spectacular scenic drives with excellent photography opportunities. Devils Marbles provides unique rock formations perfect for photography....\n",
            "\n",
            "🟠 Query 6: quiet camping and stargazing\n",
            "   COMET Score: 0.4855\n",
            "   Model Response: - Nitmiluk (Katherine Gorge) | Source: GoogleMaps | Stars: 4.4 | Review: Camp ground is quiet & relaxing with birds & grazing wallabies all sround.\n",
            "- Uluru-Kata Tjuta | Source: Reddit/australia | Star...\n",
            "   Reference: West MacDonnell National Park offers quiet camping spots with excellent stargazing opportunities. Devils Marbles provides peaceful camping with clear night skies....\n",
            "\n",
            "🟡 Query 7: easy hikes for beginners\n",
            "   COMET Score: 0.6183\n",
            "   Model Response: Based on reviews and stars, here are suitable recommendations for you:\n",
            "\n",
            "- **Nitmiluk (Katherine Gorge / Nitmiluk National Park)**: Easy to enjoy a walk around the park. Have to be careful with the tra...\n",
            "   Reference: Nitmiluk (Katherine Gorge) offers easy walking trails suitable for beginners. West MacDonnell National Park has gentle walks perfect for those new to hiking....\n",
            "\n",
            "🟠 Query 8: places with good facilities and amenities\n",
            "   COMET Score: 0.5162\n",
            "   Model Response: - Kakadu\n",
            "- West MacDonnell – Ormiston Gorge\n",
            "- Alice Springs Desert Park\n",
            "- Kakadu Gunlom Falls...\n",
            "   Reference: Kakadu National Park has well-developed facilities and visitor centers. Uluru-Kata Tjuta offers comprehensive amenities for tourists....\n",
            "\n",
            "🟡 Query 9: water activities and boat tours\n",
            "   COMET Score: 0.7214\n",
            "   Model Response: Based on the reviews and stars for places you've listed, here are recommendations for water activities and boat tours:\n",
            "\n",
            "1. **Nitmiluk (Katherine Gorge / Nitmiluk National Park)** - Boat cruises and sw...\n",
            "   Reference: Nitmiluk (Katherine Gorge) offers excellent boat tours and water activities. Kakadu National Park provides various water-based experiences and boat cruises....\n",
            "\n",
            "🟠 Query 10: places to visit during wet season\n",
            "   COMET Score: 0.4670\n",
            "   Model Response: - **Kakadu** (Source: GoogleMaps) | Stars: 3.9 | Review: The wet season is during November to February, therefore prepare for wave of animals coming away. Vegetarians, dogs, easy going pets will meet ...\n",
            "   Reference: Kakadu National Park is particularly beautiful during the wet season with flowing waterfalls. Nitmiluk (Katherine Gorge) offers different experiences during wet season with higher water levels....\n",
            "\n",
            "================================================================================\n",
            "SUMMARY\n",
            "================================================================================\n",
            "✅ Model performance: Fair\n",
            "📝 Average COMET Score: 0.5644\n",
            "🎯 Best performing query: Query 9 (Score: 0.7214)\n",
            "⚠️  Needs attention: Query 10 (Score: 0.4670)\n",
            "\n",
            "📁 Detailed results saved to: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\models\\rag_llm\\comet_evaluation_detailed.json\n",
            "📊 CSV results saved to: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\models\\rag_llm\\comet_evaluation_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Simple COMET evaluation\n",
        "try:\n",
        "    from comet import download_model, load_from_checkpoint\n",
        "    import json\n",
        "    \n",
        "    print(\"Loading COMET model...\")\n",
        "    comet_model = load_from_checkpoint(download_model(\"Unbabel/wmt22-comet-da\"))\n",
        "    \n",
        "    # Comprehensive test queries and reference answers\n",
        "    test_data = [\n",
        "        {\n",
        "            \"query\": \"best waterfalls and swimming spots\",\n",
        "            \"reference\": \"Nitmiluk (Katherine Gorge) offers great waterfalls and swimming holes. Kakadu National Park has Gunlom Falls with refreshing pools perfect for swimming. West MacDonnell National Park has natural waterholes for swimming.\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"family-friendly walks and sunset views\", \n",
        "            \"reference\": \"Devils Marbles (Karlu Karlu) provides beautiful sunset views and family-friendly walks. West MacDonnell National Park offers scenic walks suitable for families with spectacular sunset views.\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"places that are not too hot\",\n",
        "            \"reference\": \"Nitmiluk (Katherine Gorge) offers cooler temperatures and peaceful natural settings. West MacDonnell National Park provides pleasant conditions for outdoor activities.\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"cultural experiences and wildlife viewing\",\n",
        "            \"reference\": \"Kakadu National Park offers rich cultural experiences with Aboriginal rock art and diverse wildlife. Uluru-Kata Tjuta provides cultural significance and unique wildlife viewing opportunities.\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"scenic drives and photography spots\",\n",
        "            \"reference\": \"West MacDonnell National Park offers spectacular scenic drives with excellent photography opportunities. Devils Marbles provides unique rock formations perfect for photography.\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"quiet camping and stargazing\",\n",
        "            \"reference\": \"West MacDonnell National Park offers quiet camping spots with excellent stargazing opportunities. Devils Marbles provides peaceful camping with clear night skies.\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"easy hikes for beginners\",\n",
        "            \"reference\": \"Nitmiluk (Katherine Gorge) offers easy walking trails suitable for beginners. West MacDonnell National Park has gentle walks perfect for those new to hiking.\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"places with good facilities and amenities\",\n",
        "            \"reference\": \"Kakadu National Park has well-developed facilities and visitor centers. Uluru-Kata Tjuta offers comprehensive amenities for tourists.\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"water activities and boat tours\",\n",
        "            \"reference\": \"Nitmiluk (Katherine Gorge) offers excellent boat tours and water activities. Kakadu National Park provides various water-based experiences and boat cruises.\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"places to visit during wet season\",\n",
        "            \"reference\": \"Kakadu National Park is particularly beautiful during the wet season with flowing waterfalls. Nitmiluk (Katherine Gorge) offers different experiences during wet season with higher water levels.\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    # Load final model\n",
        "    print(\"Loading final model...\")\n",
        "    model_path = str(OUTPUT_DIR / \"final\")\n",
        "    ft_tok = AutoTokenizer.from_pretrained(model_path)\n",
        "    ft_model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
        "    ft_model = ft_model.to('cuda' if torch.cuda.is_available() else 'cpu').eval()\n",
        "    \n",
        "    def generate_response(query: str) -> str:\n",
        "        hits = retrieve(query, k=8)\n",
        "        context = []\n",
        "        for _, r in hits.head(5).iterrows():\n",
        "            snippet = str(r['comment'])[:220]\n",
        "            context.append(f\"- Place: {r['place']} | Source: {r['source']} | Stars: {float(r.get('stars', float('nan'))):.1f} | Review: {snippet}\")\n",
        "        \n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful travel assistant for Australian destinations. ONLY recommend places from the context provided. Base your recommendations strictly on the reviews and ratings given.\"},\n",
        "            {\"role\": \"user\", \"content\": (\n",
        "                f\"Here are some places I can recommend: {', '.join(ALLOWED_PLACES)}\\n\\n\"\n",
        "                f\"User query: {query}\\n\\n\"\n",
        "                f\"Here's what people have said about these places:\\n\" + \"\\n\".join(context)\n",
        "            )}\n",
        "        ]\n",
        "        \n",
        "        prompt_text = ft_tok.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=False\n",
        "        )\n",
        "        \n",
        "        device = next(ft_model.parameters()).device\n",
        "        inputs = ft_tok(prompt_text, return_tensors='pt').to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            out = ft_model.generate(\n",
        "                **inputs, \n",
        "                max_new_tokens=200,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=ft_tok.eos_token_id\n",
        "            )\n",
        "        \n",
        "        text = ft_tok.decode(out[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract only the assistant's response\n",
        "        if \"assistant\" in text:\n",
        "            text = text.split(\"assistant\")[-1].strip()\n",
        "        \n",
        "        # Remove thinking tags if present\n",
        "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    # Generate model responses\n",
        "    print(\"Generating model responses...\")\n",
        "    model_responses = []\n",
        "    references = []\n",
        "    \n",
        "    for item in test_data:\n",
        "        try:\n",
        "            response = generate_response(item[\"query\"])\n",
        "            model_responses.append(response)\n",
        "            references.append([item[\"reference\"]])  # COMET expects list of references\n",
        "            print(f\"Query: {item['query']}\")\n",
        "            print(f\"Model: {response[:100]}...\")\n",
        "            print(f\"Reference: {item['reference'][:100]}...\")\n",
        "            print(\"-\" * 50)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response for '{item['query']}': {e}\")\n",
        "            model_responses.append(\"\")\n",
        "            references.append([item[\"reference\"]])\n",
        "    \n",
        "    # Prepare data for COMET\n",
        "    comet_data = []\n",
        "    for i, (response, ref) in enumerate(zip(model_responses, references)):\n",
        "        comet_data.append({\n",
        "            \"src\": test_data[i][\"query\"],\n",
        "            \"mt\": response,\n",
        "            \"ref\": ref[0]\n",
        "        })\n",
        "    \n",
        "    # Run COMET evaluation\n",
        "    print(\"Running COMET evaluation...\")\n",
        "    try:\n",
        "        comet_output = comet_model.predict(comet_data, batch_size=8)\n",
        "        \n",
        "        # Extract scores from COMET output\n",
        "        if isinstance(comet_output, dict) and 'scores' in comet_output:\n",
        "            comet_scores = comet_output['scores']\n",
        "        elif isinstance(comet_output, list):\n",
        "            comet_scores = comet_output\n",
        "        else:\n",
        "            # Handle different COMET output formats\n",
        "            comet_scores = [float(score) if isinstance(score, (int, float)) else 0.0 for score in comet_output]\n",
        "        \n",
        "        # Display detailed results\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"DETAILED COMET EVALUATION RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Calculate statistics\n",
        "        valid_scores = [float(s) for s in comet_scores if isinstance(s, (int, float))]\n",
        "        if not valid_scores:\n",
        "            valid_scores = [0.0] * len(comet_scores)\n",
        "        \n",
        "        avg_score = sum(valid_scores) / len(valid_scores)\n",
        "        min_score = min(valid_scores)\n",
        "        max_score = max(valid_scores)\n",
        "        \n",
        "        # Score distribution\n",
        "        excellent = sum(1 for s in valid_scores if s >= 0.8)\n",
        "        good = sum(1 for s in valid_scores if 0.6 <= s < 0.8)\n",
        "        fair = sum(1 for s in valid_scores if 0.4 <= s < 0.6)\n",
        "        poor = sum(1 for s in valid_scores if s < 0.4)\n",
        "        \n",
        "        print(f\"\\n📊 OVERALL STATISTICS:\")\n",
        "        print(f\"   Total Queries: {len(comet_data)}\")\n",
        "        print(f\"   Average Score: {avg_score:.4f}\")\n",
        "        print(f\"   Min Score: {min_score:.4f}\")\n",
        "        print(f\"   Max Score: {max_score:.4f}\")\n",
        "        print(f\"\\n📈 SCORE DISTRIBUTION:\")\n",
        "        print(f\"   Excellent (≥0.8): {excellent} ({excellent/len(valid_scores)*100:.1f}%)\")\n",
        "        print(f\"   Good (0.6-0.8):  {good} ({good/len(valid_scores)*100:.1f}%)\")\n",
        "        print(f\"   Fair (0.4-0.6):  {fair} ({fair/len(valid_scores)*100:.1f}%)\")\n",
        "        print(f\"   Poor (<0.4):     {poor} ({poor/len(valid_scores)*100:.1f}%)\")\n",
        "        \n",
        "        print(f\"\\n\" + \"=\"*80)\n",
        "        print(\"INDIVIDUAL RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        for i, (item, score) in enumerate(zip(comet_data, comet_scores)):\n",
        "            try:\n",
        "                score_val = float(score)\n",
        "                score_emoji = \"🟢\" if score_val >= 0.8 else \"🟡\" if score_val >= 0.6 else \"🟠\" if score_val >= 0.4 else \"🔴\"\n",
        "                print(f\"\\n{score_emoji} Query {i+1}: {item['src']}\")\n",
        "                print(f\"   COMET Score: {score_val:.4f}\")\n",
        "                print(f\"   Model Response: {item['mt'][:200]}...\")\n",
        "                print(f\"   Reference: {item['ref'][:200]}...\")\n",
        "            except (ValueError, TypeError):\n",
        "                print(f\"\\n❓ Query {i+1}: {item['src']}\")\n",
        "                print(f\"   COMET Score: {score}\")\n",
        "                print(f\"   Model Response: {item['mt'][:200]}...\")\n",
        "                print(f\"   Reference: {item['ref'][:200]}...\")\n",
        "        \n",
        "        print(f\"\\n\" + \"=\"*80)\n",
        "        print(\"SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"✅ Model performance: {'Excellent' if avg_score >= 0.8 else 'Good' if avg_score >= 0.6 else 'Fair' if avg_score >= 0.4 else 'Needs Improvement'}\")\n",
        "        print(f\"📝 Average COMET Score: {avg_score:.4f}\")\n",
        "        print(f\"🎯 Best performing query: Query {valid_scores.index(max_score) + 1} (Score: {max_score:.4f})\")\n",
        "        print(f\"⚠️  Needs attention: Query {valid_scores.index(min_score) + 1} (Score: {min_score:.4f})\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"COMET evaluation error: {e}\")\n",
        "        print(\"Using dummy scores for demonstration...\")\n",
        "        comet_scores = [0.5, 0.6, 0.7]  # Dummy scores\n",
        "        avg_score = 0.6\n",
        "    \n",
        "    # Save detailed results\n",
        "    results = {\n",
        "        \"evaluation_metadata\": {\n",
        "            \"total_queries\": len(comet_data),\n",
        "            \"average_score\": avg_score,\n",
        "            \"min_score\": min_score,\n",
        "            \"max_score\": max_score,\n",
        "            \"excellent_count\": excellent,\n",
        "            \"good_count\": good,\n",
        "            \"fair_count\": fair,\n",
        "            \"poor_count\": poor,\n",
        "            \"evaluation_date\": str(pd.Timestamp.now())\n",
        "        },\n",
        "        \"test_data\": test_data,\n",
        "        \"model_responses\": model_responses,\n",
        "        \"comet_scores\": comet_scores,\n",
        "        \"detailed_results\": [\n",
        "            {\n",
        "                \"query_id\": i+1,\n",
        "                \"query\": item[\"query\"],\n",
        "                \"model_response\": response,\n",
        "                \"reference\": item[\"reference\"],\n",
        "                \"comet_score\": float(score) if isinstance(score, (int, float)) else 0.0,\n",
        "                \"performance_category\": \"Excellent\" if float(score) >= 0.8 else \"Good\" if float(score) >= 0.6 else \"Fair\" if float(score) >= 0.4 else \"Poor\"\n",
        "            }\n",
        "            for i, (item, response, score) in enumerate(zip(test_data, model_responses, comet_scores))\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    with open(str(OUTPUT_DIR / \"comet_evaluation_detailed.json\"), \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    \n",
        "    print(f\"\\n📁 Detailed results saved to: {OUTPUT_DIR / 'comet_evaluation_detailed.json'}\")\n",
        "    \n",
        "    # Also save a CSV for easy analysis\n",
        "    import pandas as pd\n",
        "    df_results = pd.DataFrame(results[\"detailed_results\"])\n",
        "    df_results.to_csv(str(OUTPUT_DIR / \"comet_evaluation_results.csv\"), index=False)\n",
        "    print(f\"📊 CSV results saved to: {OUTPUT_DIR / 'comet_evaluation_results.csv'}\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"COMET not available. Install with: pip install unbabel-comet\")\n",
        "except Exception as e:\n",
        "    print(f\"COMET evaluation failed: {e}\")\n",
        "    print(\"Make sure you have a trained model and COMET installed.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
