{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune Recommender LLM with LoRA/PEFT\n",
        "\n",
        "This notebook fine-tunes a small chat model to produce grounded, JSON-formatted recommendations using synthetic SFT data derived from our reviews + retrieval. Artifacts are saved to `models/rag_llm/` and can be used by the merged `finetune_rag_llm.ipynb` notebook.\n",
        "\n",
        "## üöÄ **Clean Workflow - Run Steps in Order:**\n",
        "\n",
        "1. **Step 0** - Setup & retrieval artifacts\n",
        "2. **Step 1** - Configuration + Convert training data to chat format\n",
        "3. **Step 2** - Build synthetic SFT dataset  \n",
        "4. **Step 3** - Tokenizer/model setup\n",
        "5. **Step 4** - **Fine-tune model** ‚ö†Ô∏è (6-8 hours)\n",
        "6. **Step 5** - Save final model\n",
        "7. **Step 6** - **Test the model** ‚úÖ\n",
        "\n",
        "## ‚ö†Ô∏è **Important Notes:**\n",
        "- **Step 4** fine-tunes the model to produce working JSON responses\n",
        "- **Step 6** tests the fine-tuned model\n",
        "- All training cells have been cleaned up for clarity\n",
        "- Model saves to `models/rag_llm/` directory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 0 ‚Äî Ensure retrieval artifacts exist\n",
        "\n",
        "- Builds FAISS index and metadata if missing (uses the same pipeline as the merged notebook)\n",
        "- Required for synthesizing SFT data from retrieval context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using labeled data at: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\data\\generate_stars\\processed\\reviews_with_stars.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38/38 [03:59<00:00,  6.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Built index and metadata.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer # type: ignore\n",
        "\n",
        "try:\n",
        "    import faiss  # type: ignore\n",
        "except ImportError:\n",
        "    raise SystemExit(\"faiss is required. Install with `pip install faiss-cpu` on Windows.\")\n",
        "\n",
        "import pyarrow.parquet as pq\n",
        "from tqdm import tqdm\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "EMBED_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "EMBED_MODEL_DIR = snapshot_download(repo_id=EMBED_MODEL)\n",
        "embedder = SentenceTransformer(EMBED_MODEL_DIR)\n",
        "\n",
        "DATA_PATH = Path('data/processed/reviews_with_stars.csv')\n",
        "PROJECT_ROOT = Path.cwd() if (Path.cwd() / 'data').exists() else Path.cwd().parent\n",
        "INDEX_DIR = PROJECT_ROOT / 'models' / 'rag_llm' / 'step_0'\n",
        "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "INDEX_PATH = INDEX_DIR / 'reviews_all-MiniLM-L6-v2.index'\n",
        "METADATA_PATH = PROJECT_ROOT / 'data' / 'rag_llm' / 'processed' / 'review_metadata.parquet'\n",
        "METADATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "MANIFEST_PATH = INDEX_DIR / 'manifest.json'\n",
        "MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "embedder = SentenceTransformer(MODEL_NAME)\n",
        "embedding_dim = embedder.get_sentence_embedding_dimension()\n",
        "\n",
        "\n",
        "def build_index(force_rebuild: bool = False):\n",
        "    needs_build = force_rebuild or not (INDEX_PATH.exists() and METADATA_PATH.exists() and MANIFEST_PATH.exists())\n",
        "    if not needs_build:\n",
        "        print('Index and metadata already exist. Skipping build.')\n",
        "        return\n",
        "    # Locate project base (folder containing data/processed), then pick labeled file\n",
        "    def _find_base_dir(start: Path) -> Path:\n",
        "        if (start / 'data' / 'generate_stars' / 'processed').exists():\n",
        "            return start\n",
        "        for parent in start.parents:\n",
        "            if (parent / 'data' / 'generate_stars' / 'processed').exists():\n",
        "                return parent\n",
        "        return start\n",
        "\n",
        "    BASE_DIR = _find_base_dir(Path.cwd())\n",
        "    candidates = [\n",
        "        BASE_DIR / 'data' / 'generate_stars' / 'processed' / 'reviews_with_stars.csv',\n",
        "        BASE_DIR / 'data' / 'generate_stars' / 'processed' / 'reviews_with_stars_trained.csv',\n",
        "    ]\n",
        "    data_path = next((p for p in candidates if p.exists()), None)\n",
        "    assert data_path is not None, f\"Missing labeled data under {BASE_DIR / 'data' / 'generate_stars' / 'processed'}. Run generate_stars.ipynb first.\"\n",
        "    print('Using labeled data at:', data_path)\n",
        "\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "    # Ensure stars_float exists (prefer float ratings). Derive from integer 'stars' if needed\n",
        "    if 'stars_float' not in df.columns:\n",
        "        if 'stars' in df.columns:\n",
        "            df['stars_float'] = pd.to_numeric(df['stars'], errors='coerce').astype(float)\n",
        "        else:\n",
        "            raise ValueError(\"No 'stars_float' or 'stars' column in labeled data.\")\n",
        "\n",
        "    # Minimal schema check for remaining fields\n",
        "    req = ['source', 'place', 'comment']\n",
        "    missing = [c for c in req if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Labeled data missing columns: {missing}\")\n",
        "    df = df.dropna(subset=['comment']).copy()\n",
        "    df['comment'] = df['comment'].astype(str).str.strip()\n",
        "    df = df[df['comment'].str.len() > 0].reset_index(drop=True)\n",
        "\n",
        "    texts = df['comment'].tolist()\n",
        "    n = len(texts)\n",
        "    embeddings = np.empty((n, embedding_dim), dtype='float32')\n",
        "    for start in tqdm(range(0, n, BATCH_SIZE), total=(n + BATCH_SIZE - 1)//BATCH_SIZE, desc=\"Embedding\"):\n",
        "        end = min(start + BATCH_SIZE, n)\n",
        "        batch = texts[start:end]\n",
        "        emb = embedder.encode(batch, batch_size=64, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
        "        embeddings[start:end] = emb\n",
        "\n",
        "    index = faiss.IndexFlatIP(embedding_dim)\n",
        "    index.add(embeddings)\n",
        "    faiss.write_index(index, str(INDEX_PATH))\n",
        "\n",
        "    metadata = df[['source', 'place', 'comment', 'stars_float']].copy()\n",
        "    metadata.insert(0, 'row_id', np.arange(len(metadata), dtype=np.int64))\n",
        "    METADATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    metadata.to_parquet(METADATA_PATH, index=False)\n",
        "\n",
        "    manifest = {\n",
        "        'model': MODEL_NAME,\n",
        "        'embedding_dim': int(embedding_dim),\n",
        "        'index_type': 'IndexFlatIP',\n",
        "        'index_path': str(INDEX_PATH),\n",
        "        'metadata_path': str(METADATA_PATH),\n",
        "        'num_vectors': int(index.ntotal)\n",
        "    }\n",
        "    with open(MANIFEST_PATH, 'w', encoding='utf-8') as f:\n",
        "        json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
        "    print('Built index and metadata.')\n",
        "\n",
        "\n",
        "# Ensure ready\n",
        "build_index(force_rebuild=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1 ‚Äî Setup & configuration\n",
        "\n",
        "- Choose a small instruction model (CPU-friendly)\n",
        "- Define output directory `models/rag_llm/`\n",
        "- Reuse allowed places + retrieval to synthesize SFT data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "try:\n",
        "    import faiss  # type: ignore\n",
        "except ImportError:\n",
        "    raise SystemExit(\"faiss is required. Install with `pip install faiss-cpu` on Windows.\")\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "# PEFT / LoRA\n",
        "try:\n",
        "    from peft import LoraConfig, get_peft_model, PeftModel\n",
        "except ImportError:\n",
        "    raise SystemExit(\"peft is required. Install with `pip install peft`.\")\n",
        "\n",
        "BASE_MODEL = 'Qwen/Qwen3-0.6B'  # Qwen 0.6B for 8GB VRAM\n",
        "OUTPUT_DIR = (Path.cwd() if (Path.cwd() / 'data').exists() else Path.cwd().parent) / 'models' / 'rag_llm'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PROJECT_ROOT = Path.cwd() if (Path.cwd() / 'data').exists() else Path.cwd().parent\n",
        "INDEX_DIR = PROJECT_ROOT / 'models' / 'rag_llm' / 'step_0'\n",
        "with open(INDEX_DIR / 'manifest.json', 'r', encoding='utf-8') as f:\n",
        "    manifest = json.load(f)\n",
        "faiss_index = faiss.read_index(manifest['index_path'])\n",
        "md_df = pq.read_table(manifest['metadata_path']).to_pandas()\n",
        "ALLOWED_PLACES = sorted(md_df['place'].dropna().unique().tolist())\n",
        "# Ensure stars_float is present in metadata\n",
        "aassert_col = 'stars_float'\n",
        "if aassert_col not in md_df.columns:\n",
        "    raise SystemExit(\"Expected 'stars_float' in metadata; re-run Step 0 with force_rebuild=True.\")\n",
        "\n",
        "# Embeddings for retrieval context\n",
        "EMBED_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "\n",
        "def retrieve(query: str, k: int = 8) -> pd.DataFrame:\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype('float32')\n",
        "    scores, idx = faiss_index.search(q_emb, k)\n",
        "    hits = []\n",
        "    for i, s in zip(idx[0], scores[0]):\n",
        "        if i == -1:\n",
        "            continue\n",
        "        row = md_df.iloc[int(i)].to_dict()\n",
        "        row['score'] = float(s)\n",
        "        # For convenience, expose a float alias for downstream\n",
        "        row['stars'] = float(row.get('stars_float', float('nan')))\n",
        "        hits.append(row)\n",
        "    return pd.DataFrame(hits)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2 ‚Äî Build synthetic SFT dataset\n",
        "\n",
        "- Create input/output pairs using retrieval context\n",
        "- Inputs: system prompt + allowed places + user query + review context\n",
        "- Targets: JSON with `recommended_places`, `reasoning`, and `citations`\n",
        "- Saves `data/rag_llm/rag_sft.jsonl`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "SFT synthesis: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:07<00:00, 129.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote SFT dataset: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\data\\rag_llm\\processed\\rag_sft.jsonl | examples: 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPT = (\n",
        "    \"You are a travel recommendation assistant for Australian destinations. \"\n",
        "    \"Recommend only places from Allowed Places. Ground answers in the context. \"\n",
        "    \"Respond with JSON: {recommended_places: [..], reasoning: str, citations: [{place, source, stars, snippet}]}\"\n",
        ")\n",
        "\n",
        "# Target size (adjustable)\n",
        "NUM_EXAMPLES_TARGET = 1000\n",
        "RNG_SEED = 42\n",
        "random.seed(RNG_SEED)\n",
        "\n",
        "# Query templates and facets\n",
        "activities = [\n",
        "    \"short hikes\", \"lookouts\", \"waterfalls\", \"swimming spots\", \"wildlife\",\n",
        "    \"cultural experiences\", \"sunset views\", \"sunrise views\", \"family-friendly walks\",\n",
        "    \"night sky views\", \"quiet camping\", \"scenic drives\"\n",
        "]\n",
        "modifiers = [\n",
        "    \"easy\", \"moderate\", \"kid-friendly\", \"photogenic\", \"less crowded\", \"near facilities\"\n",
        "]\n",
        "intents = []\n",
        "for _ in range(300):\n",
        "    a = random.choice(activities)\n",
        "    m = random.choice(modifiers)\n",
        "    intents.append(f\"{m} {a}\")\n",
        "\n",
        "# Ensure each allowed place appears by creating place-focused intents\n",
        "place_anchored = [f\"best things to do at {p}\" for p in ALLOWED_PLACES]\n",
        "USER_QUERIES = list(dict.fromkeys(intents + place_anchored))\n",
        "\n",
        "OUT_PATH = PROJECT_ROOT / 'data' / 'rag_llm' / 'processed' / 'rag_sft.jsonl'\n",
        "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def build_context(hits: pd.DataFrame, max_chars: int = 1200, max_rows: int = 8) -> str:\n",
        "    rows = []\n",
        "    used = 0\n",
        "    for _, r in hits.head(max_rows).iterrows():\n",
        "        snippet = str(r['comment'])\n",
        "        if len(snippet) > 240:\n",
        "            snippet = snippet[:240] + '...'\n",
        "        line = f\"- Place: {r['place']} | Source: {r['source']} | Stars: {float(r.get('stars', float('nan'))):.1f} | Review: {snippet}\"\n",
        "        if used + len(line) > max_chars:\n",
        "            break\n",
        "        rows.append(line)\n",
        "        used += len(line)\n",
        "    return \"\\n\".join(rows)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "num_written = 0\n",
        "max_rounds = 1000  # hard cap to avoid infinite loop if retrieval becomes empty\n",
        "with open(OUT_PATH, 'w', encoding='utf-8') as f:\n",
        "    pbar = tqdm(total=NUM_EXAMPLES_TARGET, desc='SFT synthesis')\n",
        "    rounds = 0\n",
        "    while num_written < NUM_EXAMPLES_TARGET and rounds < max_rounds:\n",
        "        # Shuffle intents each round for diversity\n",
        "        for q in random.sample(USER_QUERIES, len(USER_QUERIES)):\n",
        "            # Write up to N variants per intent until we hit the target\n",
        "            for _ in range(4):\n",
        "                if num_written >= NUM_EXAMPLES_TARGET:\n",
        "                    break\n",
        "                # Randomize retrieval by adding a small jitter and sampling top-k\n",
        "                k = 12\n",
        "                hits = retrieve(q, k=k)\n",
        "                if hits.empty:\n",
        "                    continue\n",
        "                # Shuffle to diversify citations/contexts\n",
        "                hits = hits.sample(frac=1.0, random_state=random.randint(0, 10_000)).reset_index(drop=True)\n",
        "                context = build_context(hits, max_rows=8)\n",
        "                if not context:\n",
        "                    continue\n",
        "                # Choose top places by mean stars (on the shuffled subset)\n",
        "                top_places = (\n",
        "                    hits.groupby('place')['stars']\n",
        "                        .mean()\n",
        "                        .sort_values(ascending=False)\n",
        "                        .head(3)\n",
        "                        .index.tolist()\n",
        "                )  # 'stars' is a float alias\n",
        "                # Build citations subset (best-scored after shuffle)\n",
        "                cits = []\n",
        "                for _, r in hits.head(5).iterrows():\n",
        "                    cits.append({\n",
        "                        'place': r['place'], 'source': r['source'], 'stars': round(float(r.get('stars', float('nan'))), 1),\n",
        "                        'snippet': str(r['comment'])[:220]\n",
        "                    })\n",
        "                prompt = (\n",
        "                    f\"[SYSTEM]\\n{SYSTEM_PROMPT}\\n\\n\"\n",
        "                    f\"[ALLOWED_PLACES]\\n{', '.join(ALLOWED_PLACES)}\\n\\n\"\n",
        "                    f\"[USER_QUERY]\\n{q}\\n\\n\"\n",
        "                    f\"[REVIEW_CONTEXT]\\n{context}\\n\\n\"\n",
        "                )\n",
        "                target = {\n",
        "                    'recommended_places': [p for p in top_places if p in ALLOWED_PLACES],\n",
        "                    'reasoning': 'Based on reviews and stars for relevance to the query.',\n",
        "                    'citations': cits,\n",
        "                }\n",
        "                f.write(json.dumps({'instruction': prompt, 'output': target}, ensure_ascii=False) + \"\\n\")\n",
        "                num_written += 1\n",
        "                pbar.update(1)\n",
        "                if num_written >= NUM_EXAMPLES_TARGET:\n",
        "                    break\n",
        "        rounds += 1\n",
        "    pbar.close()\n",
        "\n",
        "print('Wrote SFT dataset:', OUT_PATH, '| examples:', num_written)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3 ‚Äî Convert training data to proper chat format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting data/rag_llm/processed/rag_sft.jsonl to chat format...\n",
            "Using absolute paths:\n",
            "  Input: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\data/rag_llm/processed/rag_sft.jsonl\n",
            "  Output: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\data/rag_llm/processed/rag_sft_chat.jsonl\n",
            "Converted 100 examples...\n",
            "Converted 200 examples...\n",
            "Converted 300 examples...\n",
            "Converted 400 examples...\n",
            "Converted 500 examples...\n",
            "Converted 600 examples...\n",
            "Converted 700 examples...\n",
            "Converted 800 examples...\n",
            "Converted 900 examples...\n",
            "Converted 1000 examples...\n",
            "‚úÖ Conversion complete! Saved to c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\data/rag_llm/processed/rag_sft_chat.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def convert_to_chat_format(input_file, output_file):\n",
        "    \"\"\"Convert instruction/output format to messages format for proper fine-tuning\"\"\"\n",
        "    \n",
        "    print(f\"Converting {input_file} to chat format...\")\n",
        "    \n",
        "    # Use absolute paths to avoid working directory issues\n",
        "    if not os.path.isabs(input_file):\n",
        "        # Get the project root by looking for the data directory\n",
        "        current_dir = os.path.abspath('')\n",
        "        project_root = current_dir\n",
        "        \n",
        "        # Walk up directories until we find the data folder\n",
        "        while project_root != os.path.dirname(project_root):  # Stop at root\n",
        "            if os.path.exists(os.path.join(project_root, 'data')):\n",
        "                break\n",
        "            project_root = os.path.dirname(project_root)\n",
        "        \n",
        "        input_file = os.path.join(project_root, input_file)\n",
        "        output_file = os.path.join(project_root, output_file)\n",
        "    \n",
        "    print(f\"Using absolute paths:\")\n",
        "    print(f\"  Input: {input_file}\")\n",
        "    print(f\"  Output: {output_file}\")\n",
        "    \n",
        "    # Verify input file exists\n",
        "    if not os.path.exists(input_file):\n",
        "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
        "    \n",
        "    # Use UTF-8 encoding to handle Unicode characters\n",
        "    with open(input_file, 'r', encoding='utf-8') as f_in, open(output_file, 'w', encoding='utf-8') as f_out:\n",
        "        for line_num, line in enumerate(f_in):\n",
        "            try:\n",
        "                data = json.loads(line.strip())\n",
        "                \n",
        "                # Extract instruction and output\n",
        "                instruction = data['instruction']\n",
        "                output = data['output']\n",
        "                \n",
        "                # Convert to chat format\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a travel recommendation assistant for Australian destinations. Recommend only places from Allowed Places. Ground answers in the context. Respond with JSON: {recommended_places: [..], reasoning: str, citations: [{place, source, stars, snippet}]}\"},\n",
        "                    {\"role\": \"user\", \"content\": instruction},\n",
        "                    {\"role\": \"assistant\", \"content\": json.dumps(output)}\n",
        "                ]\n",
        "                \n",
        "                # Write new format\n",
        "                new_data = {\"messages\": messages}\n",
        "                f_out.write(json.dumps(new_data) + '\\n')\n",
        "                \n",
        "                if (line_num + 1) % 100 == 0:\n",
        "                    print(f\"Converted {line_num + 1} examples...\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Error processing line {line_num + 1}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    print(f\"‚úÖ Conversion complete! Saved to {output_file}\")\n",
        "\n",
        "# Convert the training data\n",
        "convert_to_chat_format('data/rag_llm/processed/rag_sft.jsonl', 'data/rag_llm/processed/rag_sft_chat.jsonl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4 ‚Äî Tokenizer, model, and LoRA config\n",
        "\n",
        "- Load base chat model + tokenizer\n",
        "- Attach LoRA adapters (low‚Äërank update on attention/projection layers)\n",
        "- Keep it CPU-friendly (no 8‚Äëbit quantization required)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 5,046,272 || all params: 601,096,192 || trainable%: 0.8395\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "\n",
        "# Probe module names and select safe LoRA targets\n",
        "all_names = [n for n, _ in base_model.named_modules()]\n",
        "# common candidates across LLaMA-like + MobileLLM variants\n",
        "candidates = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "selected = [n.split(\".\")[-1] for n in all_names if any(c in n.split(\".\")[-1] for c in candidates)]\n",
        "# dedupe and keep only the layer names\n",
        "selected = sorted(list({s for s in selected if s in candidates}))\n",
        "if not selected:\n",
        "    selected = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=selected,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5 ‚Äî Fine-tune model\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT**: This fine-tunes the model to produce working JSON responses.\n",
        "\n",
        "**What this does**:\n",
        "- Uses proper chat format data (rag_sft_chat.jsonl)\n",
        "- Proper training parameters for Qwen model\n",
        "- Saves to models/rag_llm/ directory\n",
        "- Will produce working JSON responses instead of garbled text\n",
        "\n",
        "**Estimated time**: 6-8 hours (6 epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data...\n",
            "Using data file: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\data/rag_llm/processed/rag_sft_chat.jsonl\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c230d939fe704c99be5991e8e689073d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 900\n",
            "Evaluation samples: 100\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a1e6842969347e5b33e8ec4bf60ca89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\TARIK\\AppData\\Local\\Temp\\ipykernel_25088\\4061389872.py:80: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting fine-tuning...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  11/2700 01:35 < 7:53:31, 0.09 it/s, Epoch 0.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting fine-tuning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[32m     94\u001b[39m final_dir = OUTPUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mfinal\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\venv\\Lib\\site-packages\\transformers\\trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\venv\\Lib\\site-packages\\transformers\\trainer.py:2672\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2665\u001b[39m context = (\n\u001b[32m   2666\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2668\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2670\u001b[39m )\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\venv\\Lib\\site-packages\\transformers\\trainer.py:4060\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4057\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4058\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4060\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import os\n",
        "\n",
        "# Load the training data\n",
        "print(\"Loading training data...\")\n",
        "\n",
        "# Resolve path to avoid working directory issues\n",
        "data_file = 'data/rag_llm/processed/rag_sft_chat.jsonl'\n",
        "if not os.path.isabs(data_file):\n",
        "    # Get the project root by looking for the data directory\n",
        "    current_dir = os.path.abspath('')\n",
        "    project_root = current_dir\n",
        "    \n",
        "    # Walk up directories until we find the data folder\n",
        "    while project_root != os.path.dirname(project_root):  # Stop at root\n",
        "        if os.path.exists(os.path.join(project_root, 'data')):\n",
        "            break\n",
        "        project_root = os.path.dirname(project_root)\n",
        "    \n",
        "    data_file = os.path.join(project_root, data_file)\n",
        "\n",
        "print(f\"Using data file: {data_file}\")\n",
        "dataset = load_dataset('json', data_files=data_file)['train']\n",
        "\n",
        "# Split into train/eval\n",
        "train_size = int(0.9 * len(dataset))\n",
        "eval_size = len(dataset) - train_size\n",
        "train_dataset = dataset.select(range(train_size))\n",
        "eval_dataset = dataset.select(range(train_size, train_size + eval_size))\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
        "\n",
        "\n",
        "# Preprocess: tokenize chat messages into input_ids\n",
        "from transformers import default_data_collator\n",
        "MAX_LEN = 1024\n",
        "\n",
        "def preprocess(example):\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False,\n",
        "        enable_thinking=False,\n",
        "    )\n",
        "    enc = tokenizer(prompt, truncation=True, max_length=MAX_LEN)\n",
        "    enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
        "    return enc\n",
        "\n",
        "dataset_tok = dataset.map(preprocess, remove_columns=dataset.column_names)\n",
        "train_tok = dataset_tok.select(range(train_size))\n",
        "eval_tok = dataset_tok.select(range(train_size, train_size + eval_size))\n",
        "\n",
        "data_collator = default_data_collator\n",
        "\n",
        "# Training arguments with better settings\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    warmup_steps=100,\n",
        "    learning_rate=1e-4,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    eval_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"tensorboard\",\n",
        "    logging_dir=str(OUTPUT_DIR / \"tb\"),\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=eval_tok,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting fine-tuning...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model\n",
        "final_dir = OUTPUT_DIR / \"final\"\n",
        "trainer.save_model(str(final_dir))\n",
        "tokenizer.save_pretrained(str(final_dir))\n",
        "\n",
        "print(\"‚úÖ Fine-tuning complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6 ‚Äî Save final model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved merged model to c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\models\\rag_llm\\final\n",
            "Saved tokenizer to c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\models\\rag_llm\\final\n"
          ]
        }
      ],
      "source": [
        "# Step 5 ‚Äî Save final model bundle (run this AFTER training)\n",
        "import os, torch\n",
        "final_dir = OUTPUT_DIR / 'final'\n",
        "final_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    from peft import PeftModel\n",
        "    merged = PeftModel.from_pretrained(base_model, OUTPUT_DIR / 'adapters')\n",
        "    merged = merged.merge_and_unload()\n",
        "    merged.save_pretrained(final_dir)\n",
        "    torch.save(merged.state_dict(), final_dir / 'model_state.pth')\n",
        "    print('Saved merged model to', final_dir)\n",
        "except Exception as e:\n",
        "    model.save_pretrained(final_dir)\n",
        "    torch.save(model.state_dict(), final_dir / 'adapters_state.pth')\n",
        "    print('Saved adapters to', final_dir)\n",
        "\n",
        "# Ensure tokenizer and configs are persisted with final\n",
        "try:\n",
        "    tokenizer.save_pretrained(final_dir)\n",
        "    print('Saved tokenizer to', final_dir)\n",
        "except Exception as e:\n",
        "    print('Tokenizer save failed:', e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7 ‚Äî Chat with model\n",
        "\n",
        "- Load the saved model from `models/rag_llm/`\n",
        "- Run a sample query through retrieval + model and print JSON\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Step 6 ‚Äî Test the model\n",
        "import torch\n",
        "import re\n",
        "\n",
        "def test_corrected_model():\n",
        "    \"\"\"Test the corrected fine-tuned model\"\"\"\n",
        "    \n",
        "    # Load corrected model\n",
        "    ft_tok = AutoTokenizer.from_pretrained(\"models/rag_llm_corrected/final\")\n",
        "    ft_model = AutoModelForCausalLM.from_pretrained(\"models/rag_llm_corrected/final\", trust_remote_code=True)\n",
        "    ft_model = ft_model.to('cuda' if torch.cuda.is_available() else 'cpu').eval()\n",
        "    \n",
        "    def ask_corrected(query: str, k: int = 8) -> str:\n",
        "        hits = retrieve(query, k=k)\n",
        "        context = []\n",
        "        for _, r in hits.head(5).iterrows():\n",
        "            snippet = str(r['comment'])[:220]\n",
        "            context.append(f\"- Place: {r['place']} | Source: {r['source']} | Stars: {float(r.get('stars', float('nan'))):.1f} | Review: {snippet}\")\n",
        "        \n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": (\n",
        "                f\"[ALLOWED_PLACES]\\n{', '.join(ALLOWED_PLACES)}\\n\\n\"\n",
        "                f\"[USER_QUERY]\\n{query}\\n\\n\"\n",
        "                f\"[REVIEW_CONTEXT]\\n\" + \"\\n\".join(context)\n",
        "            )}\n",
        "        ]\n",
        "        \n",
        "        prompt_text = ft_tok.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=False  # Disable thinking for cleaner output\n",
        "        )\n",
        "        \n",
        "        device = next(ft_model.parameters()).device\n",
        "        inputs = ft_tok(prompt_text, return_tensors='pt').to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            out = ft_model.generate(\n",
        "                **inputs, \n",
        "                max_new_tokens=200,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=ft_tok.eos_token_id\n",
        "            )\n",
        "        \n",
        "        text = ft_tok.decode(out[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract only the assistant's response\n",
        "        if \"assistant\" in text:\n",
        "            text = text.split(\"assistant\")[-1].strip()\n",
        "        \n",
        "        m = re.search(r\"\\{[\\s\\S]*\\}\\s*$\", text)\n",
        "        return m.group(0) if m else text\n",
        "    \n",
        "    # Test with sample queries\n",
        "    test_queries = [\n",
        "        \"best waterfalls and swimming spots\",\n",
        "        \"family-friendly walks and sunset views\",\n",
        "        \"places that are not too hot\"\n",
        "    ]\n",
        "    \n",
        "    print(\"ü§ñ Testing corrected fine-tuned model:\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for i, query in enumerate(test_queries, 1):\n",
        "        print(f\"\\nüó£Ô∏è  Question {i}: {query}\")\n",
        "        print(\"ü§ñ Model response:\")\n",
        "        try:\n",
        "            response = ask_corrected(query)\n",
        "            print(response)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "    \n",
        "    return ask_corrected\n",
        "\n",
        "# Run the test\n",
        "ask_corrected = test_corrected_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8 ‚Äî COMET evaluation\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
