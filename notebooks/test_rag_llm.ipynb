{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Fine-tuned RAG LLM\n",
        "\n",
        "This notebook lets you immediately test the already-trained model and existing retrieval artifacts without running any of the data generation or training steps.\n",
        "\n",
        "- Loads FAISS index and metadata from `models/rag_llm/step_0/` and `data/rag_llm/processed/review_metadata.parquet`\n",
        "- Loads fine-tuned model from `models/rag_llm/final` (or extracts from `final.zip` if needed)\n",
        "- Provides quick test queries and an optional interactive chat\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 0 — Load retrieval artifacts (index + metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 0 — Load retrieval artifacts (index + metadata)\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import faiss  # type: ignore\n",
        "except ImportError:\n",
        "    raise SystemExit(\"faiss is required. Install with `pip install faiss-cpu` on Windows.\")\n",
        "\n",
        "import pyarrow.parquet as pq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Resolve project root (folder that contains `data`)\n",
        "PROJECT_ROOT = Path.cwd() if (Path.cwd() / 'data').exists() else Path.cwd().parent\n",
        "INDEX_DIR = PROJECT_ROOT / 'models' / 'rag_llm' / 'step_0'\n",
        "MANIFEST_PATH = INDEX_DIR / 'manifest.json'\n",
        "\n",
        "if not MANIFEST_PATH.exists():\n",
        "    raise SystemExit(f\"Missing manifest at {MANIFEST_PATH}. Build artifacts with the training notebook's Step 0 once.\")\n",
        "\n",
        "with open(MANIFEST_PATH, 'r', encoding='utf-8') as f:\n",
        "    manifest = json.load(f)\n",
        "\n",
        "# Load FAISS index and metadata\n",
        "faiss_index = faiss.read_index(manifest['index_path'])\n",
        "md_df = pq.read_table(manifest['metadata_path']).to_pandas()\n",
        "\n",
        "# Sanity checks and conveniences\n",
        "if 'place' not in md_df.columns or 'comment' not in md_df.columns:\n",
        "    raise SystemExit(\"Metadata parquet missing required columns: 'place', 'comment'.\")\n",
        "\n",
        "# Ensure stars as float alias\n",
        "if 'stars_float' in md_df.columns:\n",
        "    md_df['stars'] = md_df['stars_float'].astype(float)\n",
        "elif 'stars' in md_df.columns:\n",
        "    md_df['stars'] = pd.to_numeric(md_df['stars'], errors='coerce').astype(float)\n",
        "else:\n",
        "    md_df['stars'] = np.nan\n",
        "\n",
        "ALLOWED_PLACES = sorted(md_df['place'].dropna().unique().tolist())\n",
        "\n",
        "# Embedding model for retrieval\n",
        "EMBED_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "\n",
        "\n",
        "def retrieve(query: str, k: int = 8) -> pd.DataFrame:\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype('float32')\n",
        "    scores, idx = faiss_index.search(q_emb, k)\n",
        "    hits = []\n",
        "    for i, s in zip(idx[0], scores[0]):\n",
        "        if int(i) == -1:\n",
        "            continue\n",
        "        row = md_df.iloc[int(i)].to_dict()\n",
        "        row['score'] = float(s)\n",
        "        hits.append(row)\n",
        "    return pd.DataFrame(hits)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1 — Load fine-tuned model (from models/rag_llm/final or extract from ZIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded and ready. Runtime settings:\n",
            "  - Model path: c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\models\\rag_llm\\final\n",
            "  - Transformers: 4.56.2\n",
            "  - Torch: 2.8.0+cu129\n",
            "  - Device: CUDA (cuda:0)\n",
            "  - torch_dtype: float16\n",
            "  - max_memory: 7.16GiB\n",
            "  - CUDA version: 12.9\n",
            "  - GPU name: NVIDIA GeForce RTX 5060 Laptop GPU\n",
            "  - GPU memory (GB): total=7.96, reserved=2.99, allocated=1.21\n"
          ]
        }
      ],
      "source": [
        "# Step 1 — Load fine-tuned model (from models/rag_llm/final or extract from ZIP)\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, __version__ as transformers_version\n",
        "\n",
        "MODEL_DIR = PROJECT_ROOT / 'models' / 'rag_llm' / 'final'\n",
        "ZIP_PATH = PROJECT_ROOT / 'models' / 'rag_llm' / 'final.zip'\n",
        "\n",
        "# If final folder missing but zip exists, extract just-in-time\n",
        "if not MODEL_DIR.exists() and ZIP_PATH.exists():\n",
        "    import zipfile\n",
        "    print(f\"Extracting model from {ZIP_PATH} ...\")\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "        zf.extractall(PROJECT_ROOT / 'models' / 'rag_llm')\n",
        "\n",
        "if not MODEL_DIR.exists():\n",
        "    raise SystemExit(f\"Model folder not found at {MODEL_DIR}. Ensure the trained model exists.\")\n",
        "\n",
        "# Resolve runtime settings\n",
        "using_cuda = torch.cuda.is_available()\n",
        "device = 'cuda:0' if using_cuda else 'cpu'\n",
        "load_dtype = torch.float16 if using_cuda else torch.float32\n",
        "\n",
        "# Load model and tokenizer (pin to single GPU with dynamic cap)\n",
        "ft_tok = AutoTokenizer.from_pretrained(str(MODEL_DIR))\n",
        "if using_cuda:\n",
        "    try:\n",
        "        _props = torch.cuda.get_device_properties(0)\n",
        "        _total_gib = _props.total_memory / (1024**3)\n",
        "        _headroom_gib = max(0.5, _total_gib * 0.10)  # 10% or at least 0.5 GiB headroom\n",
        "        _usable_gib = max(1.0, _total_gib - _headroom_gib)\n",
        "        max_mem = {\"cuda:0\": f\"{_usable_gib:.2f}GiB\"}\n",
        "    except Exception:\n",
        "        max_mem = {\"cuda:0\": \"7.50GiB\"}\n",
        "else:\n",
        "    max_mem = None\n",
        "\n",
        "ft_model = AutoModelForCausalLM.from_pretrained(\n",
        "    str(MODEL_DIR),\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=load_dtype,\n",
        "    device_map={\"\": 0} if using_cuda else device,\n",
        "    max_memory=max_mem\n",
        ").eval()\n",
        "\n",
        "# Runtime report\n",
        "print(\"Model loaded and ready. Runtime settings:\")\n",
        "print(f\"  - Model path: {MODEL_DIR}\")\n",
        "print(f\"  - Transformers: {transformers_version}\")\n",
        "print(f\"  - Torch: {torch.__version__}\")\n",
        "print(f\"  - Device: {'CUDA' if using_cuda else 'CPU'} ({'cuda:0' if using_cuda else device})\")\n",
        "print(f\"  - torch_dtype: {str(load_dtype).replace('torch.', '')}\")\n",
        "if using_cuda and max_mem:\n",
        "    print(f\"  - max_memory: {max_mem['cuda:0']}\")\n",
        "\n",
        "if using_cuda:\n",
        "    try:\n",
        "        dev_index = 0\n",
        "        print(f\"  - CUDA version: {torch.version.cuda}\")\n",
        "        print(f\"  - GPU name: {torch.cuda.get_device_name(dev_index)}\")\n",
        "        props = torch.cuda.get_device_properties(dev_index)\n",
        "        total = props.total_memory / (1024**3)\n",
        "        reserved = torch.cuda.memory_reserved(dev_index) / (1024**3)\n",
        "        allocated = torch.cuda.memory_allocated(dev_index) / (1024**3)\n",
        "        print(f\"  - GPU memory (GB): total={total:.2f}, reserved={reserved:.2f}, allocated={allocated:.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  - CUDA info unavailable: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2 — Helper: build context and ask_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2 — Helper: build context and ask_model()\n",
        "\n",
        "def build_context(hits: pd.DataFrame, max_rows: int = 5) -> str:\n",
        "    rows = []\n",
        "    for _, r in hits.head(max_rows).iterrows():\n",
        "        snippet = str(r['comment'])[:240]\n",
        "        rows.append(f\"- Place: {r['place']} | Source: {r.get('source', 'unknown')} | Stars: {float(r.get('stars', float('nan'))):.1f} | Review: {snippet}\")\n",
        "    return \"\\n\".join(rows) if rows else \"No context found.\"\n",
        "\n",
        "\n",
        "def normalize_place_names_in_response(response_text: str) -> str:\n",
        "    mapping = {\n",
        "        'Kakadu National Park – Gunlom Falls': 'Kakadu',\n",
        "        'Kakadu Gunlom Falls': 'Kakadu',\n",
        "        'Nitmiluk (Katherine Gorge / Nitmiluk National Park)': 'Nitmiluk (Katherine Gorge)',\n",
        "        'Tjoritja / West MacDonnell National Park': 'West MacDonnell National Park',\n",
        "        'West MacDonnell – Ormiston Gorge': 'West MacDonnell National Park',\n",
        "        'West MacDonnell Ormiston': 'West MacDonnell National Park',\n",
        "    }\n",
        "    normalized = response_text\n",
        "    for old, new in mapping.items():\n",
        "        normalized = normalized.replace(old, new)\n",
        "    return normalized\n",
        "\n",
        "\n",
        "def ask_model(query: str, k: int = 8) -> str:\n",
        "    hits = retrieve(query, k=k)\n",
        "    context = build_context(hits, max_rows=5)\n",
        "\n",
        "    # Soft switches from user input\n",
        "    user_wants_think = \"/no_think\" not in query.lower()\n",
        "    clean_query = query.replace(\"/think\", \"\").replace(\"/no_think\", \"\").strip()\n",
        "\n",
        "    # Derive top candidate places from hits by mean stars\n",
        "    top_candidates = []\n",
        "    try:\n",
        "        if not hits.empty and 'place' in hits.columns and 'stars' in hits.columns:\n",
        "            top_candidates = (\n",
        "                hits.groupby('place')['stars']\n",
        "                    .mean()\n",
        "                    .sort_values(ascending=False)\n",
        "                    .head(3)\n",
        "                    .index.tolist()\n",
        "            )\n",
        "    except Exception:\n",
        "        top_candidates = []\n",
        "\n",
        "    candidate_str = \", \".join([p for p in top_candidates if p in ALLOWED_PLACES])\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a helpful travel assistant for Australian destinations. \"\n",
        "                \"Only recommend places from Allowed Places. Base your answer ONLY on the review context. \"\n",
        "                \"Think inside <think>...</think> concisely. After </think>, write a SINGLE short paragraph starting with 'Answer:' that recommends 2–3 places.\"\n",
        "            ),\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                f\"Allowed Places: {', '.join(ALLOWED_PLACES)}\\n\\n\"\n",
        "                f\"Use these candidate places if relevant: {candidate_str if candidate_str else 'N/A'}\\n\\n\"\n",
        "                f\"User query: {clean_query}\\n\\n\"\n",
        "                f\"Review context:\\n{context}\"\n",
        "            ),\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Build prompt as text with Qwen's thinking mode toggled by user\n",
        "    prompt_text = ft_tok.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=user_wants_think,\n",
        "    )\n",
        "    device = ft_model.device\n",
        "    inputs = ft_tok(prompt_text, return_tensors=\"pt\").to(device)\n",
        "    if 'attention_mask' not in inputs:\n",
        "        inputs['attention_mask'] = torch.ones_like(inputs['input_ids'], device=device)\n",
        "\n",
        "    # Use Qwen's recommended sampling settings for thinking mode\n",
        "    with torch.no_grad():\n",
        "        out = ft_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=240,\n",
        "            do_sample=True,\n",
        "            temperature=0.6 if user_wants_think else 0.7,\n",
        "            top_p=0.95 if user_wants_think else 0.8,\n",
        "            top_k=20,\n",
        "            repetition_penalty=1.15 if user_wants_think else 1.1,\n",
        "            pad_token_id=ft_tok.eos_token_id,\n",
        "            eos_token_id=ft_tok.eos_token_id,\n",
        "            use_cache=True,\n",
        "        )\n",
        "\n",
        "    full = ft_tok.decode(out[0], skip_special_tokens=True)\n",
        "    if \"assistant\" in full:\n",
        "        full = full.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    # Parse <think>...</think> and answer content\n",
        "    import re as _re\n",
        "    think_match = _re.search(r\"<think>(.*?)</think>\", full, flags=_re.DOTALL)\n",
        "    thinking = think_match.group(1).strip() if think_match else \"\"\n",
        "\n",
        "    # Prefer explicit 'Answer:' section if present\n",
        "    answer_match = _re.search(r\"Answer:\\s*(.+)$\", full, flags=_re.DOTALL)\n",
        "    if answer_match:\n",
        "        answer = answer_match.group(1).strip()\n",
        "    else:\n",
        "        answer = full[think_match.end():].strip() if think_match else full.strip()\n",
        "\n",
        "    # Sanitize any residual thinking tags from the answer\n",
        "    answer = _re.sub(r\"<think>.*?</think>\", \"\", answer, flags=_re.DOTALL)\n",
        "    answer = answer.replace(\"</think>\", \"\").replace(\"<think>\", \"\").strip()\n",
        "\n",
        "    # If answer is missing/too short, re-prompt once with thinking disabled for a concise answer\n",
        "    if len(answer) < 30:\n",
        "        retry_messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You are a helpful travel assistant. Provide a single concise paragraph answer only. \"\n",
        "                    \"Do not include think tags or reasoning.\"\n",
        "                ),\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": (\n",
        "                    f\"From these Allowed Places: {', '.join(ALLOWED_PLACES)}, and candidates: {candidate_str if candidate_str else 'N/A'}.\\n\"\n",
        "                    f\"User query: {query}\\n\\nReview context:\\n{context}\\n\\n\"\n",
        "                    \"Write 1–2 sentences recommending the top 2–3 places, grounded in the context.\"\n",
        "                ),\n",
        "            },\n",
        "        ]\n",
        "        retry_prompt = ft_tok.apply_chat_template(\n",
        "            retry_messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=False,\n",
        "        )\n",
        "        retry_inputs = ft_tok(retry_prompt, return_tensors=\"pt\").to(device)\n",
        "        if 'attention_mask' not in retry_inputs:\n",
        "            retry_inputs['attention_mask'] = torch.ones_like(retry_inputs['input_ids'], device=device)\n",
        "        with torch.no_grad():\n",
        "            retry_out = ft_model.generate(\n",
        "                **retry_inputs,\n",
        "                max_new_tokens=150,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.95,\n",
        "                top_k=20,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=ft_tok.eos_token_id,\n",
        "                eos_token_id=ft_tok.eos_token_id,\n",
        "                use_cache=True,\n",
        "            )\n",
        "        answer = ft_tok.decode(retry_out[0], skip_special_tokens=True)\n",
        "        if \"assistant\" in answer:\n",
        "            answer = answer.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    # Clean thinking (trim, collapse whitespace)\n",
        "    thinking_clean = _re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", thinking).strip()\n",
        "\n",
        "    return (\n",
        "        f\"🧠 THINKING:\\n{thinking_clean if thinking_clean else 'N/A'}\\n\\n\"\n",
        "        f\"💬 ANSWER:\\n{answer.strip()}\\n\\n\"\n",
        "        f\"🔎 CONTEXT USED:\\n{context}\"\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Two-stage inference: Stage 1 (plan with thinking) -> Stage 2 (answer only)\n",
        "\n",
        "import re as _re2\n",
        "from typing import Dict, Any\n",
        "\n",
        "\n",
        "def first_stage_plan(query: str, k: int = 8) -> Dict[str, Any]:\n",
        "    \"\"\"Run a first pass that retrieves, builds context, and lets the model think.\n",
        "    Returns a dict with 'thinking', 'context', 'candidate_str', and 'raw_answer'.\n",
        "    \"\"\"\n",
        "    hits = retrieve(query, k=k)\n",
        "    context = build_context(hits, max_rows=5)\n",
        "\n",
        "    # Derive top candidates from hits by mean stars (same logic as ask_model)\n",
        "    top_candidates = []\n",
        "    try:\n",
        "        if not hits.empty and 'place' in hits.columns and 'stars' in hits.columns:\n",
        "            top_candidates = (\n",
        "                hits.groupby('place')['stars']\n",
        "                .mean()\n",
        "                .sort_values(ascending=False)\n",
        "                .head(3)\n",
        "                .index.tolist()\n",
        "            )\n",
        "    except Exception:\n",
        "        top_candidates = []\n",
        "\n",
        "    candidate_str = \", \".join([p for p in top_candidates if p in ALLOWED_PLACES])\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a helpful travel assistant for Australian destinations. \"\n",
        "                \"Only recommend places from Allowed Places. Base your answer ONLY on the review context. \"\n",
        "                \"Think inside <think>...</think> concisely. After </think>, write a SINGLE short paragraph starting with 'Answer:' that recommends 2–3 places.\"\n",
        "            ),\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                f\"Allowed Places: {', '.join(ALLOWED_PLACES)}\\n\\n\"\n",
        "                f\"Use these candidate places if relevant: {candidate_str if candidate_str else 'N/A'}\\n\\n\"\n",
        "                f\"User query: {query}\\n\\n\"\n",
        "                f\"Review context:\\n{context}\"\n",
        "            ),\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    prompt_text = ft_tok.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=True,\n",
        "    )\n",
        "\n",
        "    device = ft_model.device\n",
        "    inputs = ft_tok(prompt_text, return_tensors=\"pt\").to(device)\n",
        "    if 'attention_mask' not in inputs:\n",
        "        inputs['attention_mask'] = torch.ones_like(inputs['input_ids'], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = ft_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=240,\n",
        "            do_sample=True,\n",
        "            temperature=0.6,\n",
        "            top_p=0.95,\n",
        "            top_k=20,\n",
        "            repetition_penalty=1.15,\n",
        "            pad_token_id=ft_tok.eos_token_id,\n",
        "            eos_token_id=ft_tok.eos_token_id,\n",
        "            use_cache=True,\n",
        "        )\n",
        "\n",
        "    full = ft_tok.decode(out[0], skip_special_tokens=True)\n",
        "    if \"assistant\" in full:\n",
        "        full = full.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    think_match = _re2.search(r\"<think>(.*?)</think>\", full, flags=_re2.DOTALL)\n",
        "    thinking = think_match.group(1).strip() if think_match else \"\"\n",
        "\n",
        "    answer_match = _re2.search(r\"Answer:\\s*(.+)$\", full, flags=_re2.DOTALL)\n",
        "    raw_answer = answer_match.group(1).strip() if answer_match else full.strip()\n",
        "\n",
        "    thinking_clean = _re2.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", thinking).strip()\n",
        "\n",
        "    return {\n",
        "        \"thinking\": thinking_clean,\n",
        "        \"context\": context,\n",
        "        \"candidate_str\": candidate_str,\n",
        "        \"raw_answer\": raw_answer,\n",
        "    }\n",
        "\n",
        "\n",
        "def second_stage_answer(query: str, stage1: Dict[str, Any]) -> str:\n",
        "    \"\"\"Run a second pass that consumes stage1 thinking + context and outputs only the final answer.\n",
        "    Returns the cleaned, concise answer text.\n",
        "    \"\"\"\n",
        "    thinking = stage1.get(\"thinking\", \"\").strip()\n",
        "    context = stage1.get(\"context\", \"\").strip()\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a helpful travel assistant. Provide ONLY a concise final answer. \"\n",
        "                \"Do not include think tags, step-by-step plans, or prefaces. Start directly with 'Answer:' and write 1–2 sentences recommending 2–3 places grounded in the provided context.\"\n",
        "            ),\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                f\"User query: {query}\\n\\n\"\n",
        "                f\"Model planning notes (for your input only, do NOT echo):\\n{thinking}\\n\\n\"\n",
        "                f\"Review context (source of truth):\\n{context}\\n\\n\"\n",
        "                \"Write only the final answer paragraph.\"\n",
        "            ),\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    prompt_text = ft_tok.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False,\n",
        "    )\n",
        "\n",
        "    device = ft_model.device\n",
        "    inputs = ft_tok(prompt_text, return_tensors=\"pt\").to(device)\n",
        "    if 'attention_mask' not in inputs:\n",
        "        inputs['attention_mask'] = torch.ones_like(inputs['input_ids'], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = ft_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=180,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            top_k=20,\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=ft_tok.eos_token_id,\n",
        "            eos_token_id=ft_tok.eos_token_id,\n",
        "            use_cache=True,\n",
        "        )\n",
        "\n",
        "    text = ft_tok.decode(out[0], skip_special_tokens=True)\n",
        "    if \"assistant\" in text:\n",
        "        text = text.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    # Strip any stray think tags and normalize known place names\n",
        "    text = _re2.sub(r\"<think>.*?</think>\", \"\", text, flags=_re2.DOTALL).strip()\n",
        "    text = normalize_place_names_in_response(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def ask_model_two_stage(query: str, k: int = 8) -> Dict[str, str]:\n",
        "    \"\"\"Convenience wrapper that returns a dict with keys: thinking, context, answer.\"\"\"\n",
        "    stage1 = first_stage_plan(query, k=k)\n",
        "    final_answer = second_stage_answer(query, stage1)\n",
        "    return {\n",
        "        \"thinking\": stage1[\"thinking\"] or \"N/A\",\n",
        "        \"context\": stage1[\"context\"],\n",
        "        \"answer\": final_answer.strip(),\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3 — Quick tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Testing fine-tuned model (two-stage):\n",
            "============================================================\n",
            "\n",
            "🗣️ Query 1: best waterfalls and swimming spots\n",
            "🤖 Response (two-stage):\n",
            "🧠 THINKING:\n",
            "Okay, the user is asking about the best waterfalls and swimming spots. Let me look through the allowed places. There's Alice Springs Desert Park, Kakadu, Nitmiluk (Katherine Gorge), and others like Uluru-Kata Tjuta, Tjoritja / West MacDonnell National Park, etc., but primarily focus on waterfalls.\n",
            "\n",
            "Looking at reviews: Nitmiluk (Katherine Gorge) has a swimming hole. Kakadu also has a waterfall. Kakadu National Park – Gunlom Falls is even better with awesome views and a swimming pool. Tjoritja / West MacDonnell also has waterfalls. West MacDonnell – Ormiston Gorge might be another option. Need to make sure these are indeed famous spots. Yes, those seem right. So recommend Nitmiluk as a general good ride, Gunlom as a specific one, and maybe Tjoritja as well.\n",
            "\n",
            "🔎 CONTEXT USED:\n",
            "- Place: Kakadu | Source: GoogleMaps | Stars: 4.5 | Review: Lovely, there are secrete swimming spots with waterfalls throughout.\n",
            "- Place: Nitmiluk (Katherine Gorge) | Source: GoogleMaps | Stars: 4.9 | Review: Great waterfalls and swimming holes. Natural beauty everywhere you look.\n",
            "- Place: Kakadu | Source: GoogleMaps | Stars: 4.6 | Review: Great place to visit after the wet season and the waterfalls have opened back up, not much to do in the wet at all\n",
            "- Place: Kakadu National Park – Gunlom Falls | Source: TripAdvisor | Stars: 4.9 | Review: FANTASTIC!! When you get there you are greeted with a fantastic almost circular gorge, waterfall and pristine crystal clear water. A perfect place for a swim. Bring a noodle or a float of some kind because it is just great to be able to flo\n",
            "- Place: Kakadu | Source: GoogleMaps | Stars: 4.9 | Review: The best place to visit. Amazing nature Amazing water falls everywhere beautiful\n",
            "\n",
            "💬 ANSWER (final):\n",
            "Swim in the Orange River or Katherine Gorge hydropools. Let the waves take their sweet at you!\n",
            "\n",
            "============================================================\n",
            "\n",
            "🗣️ Query 2: family-friendly walks and sunset views\n",
            "🤖 Response (two-stage):\n",
            "🧠 THINKING:\n",
            "Okay, the user wants recommendations for walks and sunset views based on specific reviews. Let me look through the allowed places. Uluru-Kata Tjuta is in the list, and it's rated 4.3 out of 5 stars. That's pretty good. I'll mention that as a recommendation. Next, there's Tjoritja / West MacDonnell National Park, which has walking trails. I'll add that too. Both places have scenic drives or walks that show the beauty of the locations. Need to keep each suggestion concise so they're easy to read. Also, make sure not to use any markdown. Just be friendly and clear.\n",
            "\n",
            "🔎 CONTEXT USED:\n",
            "- Place: Uluru-Kata Tjuta | Source: GoogleMaps | Stars: 4.7 | Review: Outstanding views especially at sunset.\n",
            "- Place: Devils Marbles (Karlu Karlu) | Source: TripAdvisor | Stars: 4.5 | Review: Both sunrise and sunset are beautiful, get your camera ready for some great photos. There are some walks as well.\n",
            "- Place: Uluru-Kata Tjuta | Source: TripAdvisor | Stars: 4.3 | Review: Absolutely breathtaking to see at sunset the colours are amazing but extremely busy at this time of year we did some of the shorter walks really good.\n",
            "- Place: Devils Marbles (Karlu Karlu) | Source: GoogleMaps | Stars: 4.5 | Review: The sunset's are a must do\n",
            "- Place: Uluru-Kata Tjuta | Source: TripAdvisor | Stars: 4.9 | Review: We got to walk around the wonderful sight all day and stay for a sunset. The colors and setting is just beautiful. It's amazing to see if change colors right before your eyes. You must experience this amazing place.\n",
            "\n",
            "💬 ANSWER (final):\n",
            "Answer: Explore Uluru-Kata Tjuta for stunning walks and admire sunset views. Visit West MacDonnell National Park for unforgettable walks. 🌟\n",
            "\n",
            "============================================================\n",
            "\n",
            "🗣️ Query 3: places that are not too hot\n",
            "🤖 Response (two-stage):\n",
            "🧠 THINKING:\n",
            "Okay, the user asked for places that aren't too hot, and the allowed places include things like Nitmiluk (Katherine Gorge), Kakadu, and others. I need to make sure I'm recommending only those places. Let me think about what's reasonable. Nitmiluk is pretty cool and doesn't feel hot. Kakadu is okay as well. Tjoritja/West MacDonnell also have ranges that can be visited without being overly hot. I'll go with those.\n",
            "\n",
            "🔎 CONTEXT USED:\n",
            "- Place: Tjoritja / West MacDonnell National Park | Source: GoogleMaps | Stars: 4.6 | Review: A wonderful place to visit especially when it's hot\n",
            "- Place: Devils Marbles (Karlu Karlu) | Source: GoogleMaps | Stars: 3.4 | Review: Amazing place but it's way too hot 🥵 …\n",
            "- Place: Kakadu | Source: GoogleMaps | Stars: 2.7 | Review: Many nice places. But.. Many places closed during the wet season. (April and no water anywhere by the way)\n",
            "- Place: Kakadu | Source: GoogleMaps | Stars: 4.9 | Review: Best place ever\n",
            "- Place: Alice Springs Desert Park | Source: Reddit/AskAnAustralian | Stars: 2.5 | Review: Anywhere but Alice Springs. Rocky has its issues and you will learn a new definition of heat, but at least it’s not Alice.\n",
            "\n",
            "💬 ANSWER (final):\n",
            "Answer: Nitmiluk, West MacDonnell National Park.\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Step 3 — Quick tests (two-stage)\n",
        "print(\"🤖 Testing fine-tuned model (two-stage):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Remove explicit /think flags from queries; stage1 always enables thinking\n",
        "test_queries = [\n",
        "    \"best waterfalls and swimming spots\",\n",
        "    \"family-friendly walks and sunset views\",\n",
        "    \"places that are not too hot\",\n",
        "]\n",
        "\n",
        "for i, q in enumerate(test_queries, 1):\n",
        "    print(f\"\\n🗣️ Query {i}: {q}\")\n",
        "    print(\"🤖 Response (two-stage):\")\n",
        "    try:\n",
        "        result = ask_model_two_stage(q)\n",
        "        print(\"🧠 THINKING:\\n\" + (result[\"thinking\"] or \"N/A\"))\n",
        "        print(\"\\n🔎 CONTEXT USED:\\n\" + result[\"context\"])\n",
        "        print(\"\\n💬 ANSWER (final):\\n\" + result[\"answer\"])\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4 — Interactive chat\n",
        "\n",
        "The chat now uses a two-stage flow automatically: it first plans with concise thinking and shows the context, then produces a clean final answer. No need for /think or /no_think flags.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎯 Interactive mode - Ask your own questions\n",
            "Type 'quit' to exit\n",
            "\n",
            "🗣️ User: which place is better for walking\n",
            "\n",
            "🧠 THINKING:\n",
            "Okay, the user is asking which place is better for walking based on reviews. Let me look through the allowed places.\n",
            "\n",
            "First, check each review's place and stars. Uluru-Kata Tjuta has a 4.8 star. The walk is generally praised, though some people might find it overrated. Then there's Alice Springs Desert Park, where I would rate it based on the information provided. Wait, actually looking back, the original question was about which place is better for walking, and the reviews don't directly compare places. However, considering the closest related places, Uluru-Kata Tjuta seems to be the main recommendation here. Nitmiluk (Katherine Gorge) also gets mentions, but that's closer in style. Kakadu National Park – Gunlom Falls is another option. Since the user wants recommendations, I'll go with Uluru-Kata Tjuta as the best fit based on reviews that highlight ease of walking and scenic beauty.\n",
            "\n",
            "🔎 CONTEXT USED:\n",
            "- Place: Uluru-Kata Tjuta | Source: GoogleMaps | Stars: 4.8 | Review: Amazing you must visit can do walks at your own pace great variety of activities different level walking depending on how long and your fitness level\n",
            "- Place: Uluru-Kata Tjuta | Source: TripAdvisor | Stars: 4.0 | Review: Impressed by the size, and changing views, - always something different to see as you travel around, but cooler weather would make the walk much more enjoyable.\n",
            "- Place: Uluru-Kata Tjuta | Source: GoogleMaps | Stars: 4.8 | Review: What an awe inspiring place. Really good walking tracks/paths and the viewing places/platforms are absolutely perfect. (some even have free Wi-Fi)\n",
            "- Place: Uluru-Kata Tjuta | Source: GoogleMaps | Stars: 4.7 | Review: Great place to walk, explore and photograph.\n",
            "- Place: Kakadu National Park – Gunlom Falls | Source: TripAdvisor | Stars: 4.8 | Review: Do not miss this place - absolutely one of the most superb places I've ever been to. Dont be put off by the steep walk - they've graded the track with a machine so its a pretty smooth path for most of the way but there is a steep rocky bit \n",
            "\n",
            "💬 ANSWER (final):\n",
            "Answer: Alice Springs Desert Park and Uluru-Kata Tjuta are both excellent choices for walkers value for depth.\n"
          ]
        }
      ],
      "source": [
        "# Step 4 — Interactive chat (two-stage)\n",
        "\n",
        "def interactive_chat():\n",
        "    print(\"\\n🎯 Interactive mode - Ask your own questions\")\n",
        "    print(\"Type 'quit' to exit\")\n",
        "    while True:\n",
        "        user_query = input(\"\\n🗣️ Your question: \").strip()\n",
        "        if user_query.lower() in {\"quit\", \"exit\", \"q\"}:\n",
        "            break\n",
        "        if not user_query:\n",
        "            continue\n",
        "        try:\n",
        "            print(f\"\\n🗣️ User: {user_query}\")\n",
        "            result = ask_model_two_stage(user_query)\n",
        "            print(\"\\n🧠 THINKING:\\n\" + (result[\"thinking\"] or \"N/A\"))\n",
        "            print(\"\\n🔎 CONTEXT USED:\\n\" + result[\"context\"])\n",
        "            # Show answer only (no trimming to sentences; already concise)\n",
        "            print(\"\\n💬 ANSWER (final):\\n\" + result[\"answer\"])\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")\n",
        "\n",
        "interactive_chat()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5 — COMET evaluation\n",
        "\n",
        "Simple evaluation using COMET to score model responses against reference answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading COMET model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\TARIK\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
            "Encoder model frozen.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading final model (reusing session)...\n",
            "Generating deterministic model responses (two-stage)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\TARIK\\Desktop\\Charles Darwin University\\4 - Year 1 - Semester 2\\IT CODE FAIR\\Data Science Challenge\\venv\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ace34b7b2874ea59f4134b9e527ddbf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: best waterfalls and swimming spots\n",
            "Model (len=22): Answer: Kakadu and West MacDonnell National Park are excellent waterfalls and swimming spots. Nitmiluk (Katherine Gorge)...\n",
            "Reference: Nitmiluk (Katherine Gorge) offers great waterfalls and swimming holes. Kakadu has Gunlom Falls with refreshing pools per...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20a9d8d727ea487f8068300037b462b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: family-friendly walks and sunset views\n",
            "Model (len=23): - Visit Uluru-Kata Tjuta for unforgettable walks and sunset views. - Explore Devils Marbles (Karlu Karlu) for a relaxing...\n",
            "Reference: Devils Marbles (Karlu Karlu) provides beautiful sunset views and family-friendly walks. West MacDonnell National Park of...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c627f6c093b24c6b8ccdc141f6298668",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: places that are not too hot\n",
            "Model (len=14): Answer: West MacDonnell National Park and Kakadu are not too hot places to stay....\n",
            "Reference: Nitmiluk (Katherine Gorge) offers cooler temperatures and peaceful natural settings. West MacDonnell National Park provi...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a84187d6527f40cd8a63e169b42ab179",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: cultural experiences and wildlife viewing\n",
            "Model (len=17): Answer: Alice Springs Desert Park and Alice Springs National Park offer unique cultural experiences and stunning wildlif...\n",
            "Reference: Kakadu offers rich cultural experiences with Aboriginal rock art and diverse wildlife. Uluru-Kata Tjuta provides cultura...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a3d235459a540709625c5c72eaf9e60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: scenic drives and photography spots\n",
            "Model (len=10): photography spots along scenic drives: Uluru-Kata Tjuta, Nitmiluk (Katherine Gorge)...\n",
            "Reference: West MacDonnell National Park offers spectacular scenic drives with excellent photography opportunities. Devils Marbles ...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e71345509d2642babb4179b3e1824fb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: quiet camping and stargazing\n",
            "Model (len=33): - Uluru-Kata Tjuta camping grounds offer a unique environment for both camping and stargazing. - West MacDonnell Nationa...\n",
            "Reference: West MacDonnell National Park offers quiet camping spots with excellent stargazing opportunities. Devils Marbles provide...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b62daf72860c429e81e704d1428b1776",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Step 9c — Deterministic COMET Evaluation using Two-Stage Answers\n",
        "\n",
        "# Deterministic COMET evaluation (Stage 2 only) with place normalization\n",
        "try:\n",
        "    from comet import download_model, load_from_checkpoint\n",
        "    import json, re, random\n",
        "    import numpy as _np\n",
        "\n",
        "    # Define OUTPUT_DIR locally for saving results\n",
        "    OUTPUT_DIR = PROJECT_ROOT / 'models' / 'rag_llm'\n",
        "\n",
        "    # Set global seeds for reproducibility\n",
        "    random.seed(42)\n",
        "    _np.random.seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    print(\"Loading COMET model...\")\n",
        "    comet_model = load_from_checkpoint(download_model(\"Unbabel/wmt22-comet-da\"))\n",
        "\n",
        "    # Comprehensive test queries and reference answers\n",
        "    test_data = [\n",
        "        {\"query\": \"best waterfalls and swimming spots\", \"reference\": \"Nitmiluk (Katherine Gorge) offers great waterfalls and swimming holes. Kakadu has Gunlom Falls with refreshing pools perfect for swimming. West MacDonnell National Park has natural waterholes for swimming.\"},\n",
        "        {\"query\": \"family-friendly walks and sunset views\", \"reference\": \"Devils Marbles (Karlu Karlu) provides beautiful sunset views and family-friendly walks. West MacDonnell National Park offers scenic walks suitable for families with spectacular sunset views.\"},\n",
        "        {\"query\": \"places that are not too hot\", \"reference\": \"Nitmiluk (Katherine Gorge) offers cooler temperatures and peaceful natural settings. West MacDonnell National Park provides pleasant conditions for outdoor activities.\"},\n",
        "        {\"query\": \"cultural experiences and wildlife viewing\", \"reference\": \"Kakadu offers rich cultural experiences with Aboriginal rock art and diverse wildlife. Uluru-Kata Tjuta provides cultural significance and unique wildlife viewing opportunities.\"},\n",
        "        {\"query\": \"scenic drives and photography spots\", \"reference\": \"West MacDonnell National Park offers spectacular scenic drives with excellent photography opportunities. Devils Marbles provides unique rock formations perfect for photography.\"},\n",
        "        {\"query\": \"quiet camping and stargazing\", \"reference\": \"West MacDonnell National Park offers quiet camping spots with excellent stargazing opportunities. Devils Marbles provides peaceful camping with clear night skies.\"},\n",
        "        {\"query\": \"easy hikes for beginners\", \"reference\": \"Nitmiluk (Katherine Gorge) offers easy walking trails suitable for beginners. West MacDonnell National Park has gentle walks perfect for those new to hiking.\"},\n",
        "        {\"query\": \"places with good facilities and amenities\", \"reference\": \"Kakadu has well-developed facilities and visitor centers. Uluru-Kata Tjuta offers comprehensive amenities for tourists.\"},\n",
        "        {\"query\": \"water activities and boat tours\", \"reference\": \"Nitmiluk (Katherine Gorge) offers excellent boat tours and water activities. Kakadu provides various water-based experiences and boat cruises.\"},\n",
        "        {\"query\": \"places to visit during wet season\", \"reference\": \"Kakadu is particularly beautiful during the wet season with flowing waterfalls. Nitmiluk (Katherine Gorge) offers different experiences during wet season with higher water levels.\"}\n",
        "    ]\n",
        "\n",
        "    # Deterministic generation helpers (two-stage)\n",
        "    def _first_stage_plan_deterministic(query: str, k: int = 8) -> dict:\n",
        "        hits = retrieve(query, k=k)\n",
        "        context = build_context(hits, max_rows=5)\n",
        "\n",
        "        # Build messages mirroring first_stage_plan but we'll decode deterministically\n",
        "        top_candidates = []\n",
        "        try:\n",
        "            if not hits.empty and 'place' in hits.columns and 'stars' in hits.columns:\n",
        "                top_candidates = (\n",
        "                    hits.groupby('place')['stars']\n",
        "                    .mean()\n",
        "                    .sort_values(ascending=False)\n",
        "                    .head(3)\n",
        "                    .index.tolist()\n",
        "                )\n",
        "        except Exception:\n",
        "            top_candidates = []\n",
        "        candidate_str = \", \".join([p for p in top_candidates if p in ALLOWED_PLACES])\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": (\n",
        "                \"You are a helpful travel assistant for Australian destinations. \"\n",
        "                \"Only recommend places from Allowed Places. Base your answer ONLY on the review context. \"\n",
        "                \"Think inside <think>...</think> concisely. After </think>, write a SINGLE short paragraph starting with 'Answer:' that recommends 2–3 places.\"\n",
        "            )},\n",
        "            {\"role\": \"user\", \"content\": (\n",
        "                f\"Allowed Places: {', '.join(ALLOWED_PLACES)}\\n\\n\"\n",
        "                f\"Use these candidate places if relevant: {candidate_str if candidate_str else 'N/A'}\\n\\n\"\n",
        "                f\"User query: {query}\\n\\n\"\n",
        "                f\"Review context:\\n{context}\"\n",
        "            )}\n",
        "        ]\n",
        "\n",
        "        prompt_text = ft_tok.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=True,\n",
        "        )\n",
        "        device = ft_model.device\n",
        "        inputs = ft_tok(prompt_text, return_tensors='pt').to(device)\n",
        "        if 'attention_mask' not in inputs:\n",
        "            inputs['attention_mask'] = torch.ones_like(inputs['input_ids'], device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = ft_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=220,\n",
        "                do_sample=False,\n",
        "                num_beams=4,\n",
        "                length_penalty=1.0,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=ft_tok.eos_token_id,\n",
        "                eos_token_id=ft_tok.eos_token_id,\n",
        "                use_cache=True,\n",
        "            )\n",
        "        # Decode only newly generated tokens (exclude prompt)\n",
        "        gen_ids = out[0][inputs[\"input_ids\"].shape[1]:]\n",
        "        full = ft_tok.decode(gen_ids, skip_special_tokens=True)\n",
        "        # Robustly extract think and raw answer\n",
        "        think_match = re.search(r\"<think>(.*?)</think>\", full, flags=re.DOTALL)\n",
        "        thinking = think_match.group(1).strip() if think_match else \"\"\n",
        "        answer_match = re.search(r\"Answer:\\s*(.+)$\", full, flags=re.DOTALL)\n",
        "        raw_answer = answer_match.group(1).strip() if answer_match else full.strip()\n",
        "        thinking = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", thinking).strip()\n",
        "        return {\"thinking\": thinking, \"context\": context, \"raw_answer\": raw_answer}\n",
        "\n",
        "    def _second_stage_answer_deterministic(query: str, stage1: dict) -> str:\n",
        "        thinking = stage1.get('thinking', '').strip()\n",
        "        context = stage1.get('context', '').strip()\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": (\n",
        "                \"You are a helpful travel assistant. Provide ONLY a concise final answer. \"\n",
        "                \"Do not include think tags, step-by-step plans, or prefaces. Start directly with 'Answer:' and write 1–2 sentences recommending 2–3 places grounded in the provided context.\"\n",
        "            )},\n",
        "            {\"role\": \"user\", \"content\": (\n",
        "                f\"User query: {query}\\n\\n\"\n",
        "                f\"Model planning notes (for your input only, do NOT echo):\\n{thinking}\\n\\n\"\n",
        "                f\"Review context (source of truth):\\n{context}\\n\\n\"\n",
        "                \"Write only the final answer paragraph.\"\n",
        "            )}\n",
        "        ]\n",
        "        prompt_text = ft_tok.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=False,\n",
        "        )\n",
        "        device = ft_model.device\n",
        "        inputs = ft_tok(prompt_text, return_tensors='pt').to(device)\n",
        "        if 'attention_mask' not in inputs:\n",
        "            inputs['attention_mask'] = torch.ones_like(inputs['input_ids'], device=device)\n",
        "        with torch.no_grad():\n",
        "            out = ft_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=140,\n",
        "                do_sample=False,\n",
        "                num_beams=4,\n",
        "                length_penalty=1.0,\n",
        "                repetition_penalty=1.05,\n",
        "                pad_token_id=ft_tok.eos_token_id,\n",
        "                eos_token_id=ft_tok.eos_token_id,\n",
        "                use_cache=True,\n",
        "            )\n",
        "        # Decode only newly generated tokens (exclude prompt)\n",
        "        gen_ids = out[0][inputs[\"input_ids\"].shape[1]:]\n",
        "        text = ft_tok.decode(gen_ids, skip_special_tokens=True)\n",
        "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
        "        # Keep it short: first 2 sentences to better match references\n",
        "        sents = re.split(r\"(?<=[.!?])\\s+\", text)\n",
        "        text = \" \".join(sents[:2]).strip()\n",
        "        text = normalize_place_names_in_response(text)\n",
        "        return text\n",
        "\n",
        "    # Load final model (reuse already loaded ft_tok/ft_model if present)\n",
        "    print(\"Loading final model (reusing session)...\")\n",
        "    # ft_tok, ft_model already loaded earlier in the notebook\n",
        "\n",
        "    print(\"Generating deterministic model responses (two-stage)...\")\n",
        "    model_responses = []\n",
        "    lengths = []\n",
        "\n",
        "    for item in test_data:\n",
        "        try:\n",
        "            s1 = _first_stage_plan_deterministic(item['query'])\n",
        "            resp = _second_stage_answer_deterministic(item['query'], s1)\n",
        "            model_responses.append(resp)\n",
        "            lengths.append(len(resp.split()))\n",
        "            print(f\"Query: {item['query']}\")\n",
        "            print(f\"Model (len={lengths[-1]}): {resp[:120]}...\")\n",
        "            print(f\"Reference: {item['reference'][:120]}...\")\n",
        "            print(\"-\" * 50)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response for '{item['query']}': {e}\")\n",
        "            model_responses.append(\"\")\n",
        "            lengths.append(0)\n",
        "\n",
        "    # Prepare data for COMET\n",
        "    comet_data = []\n",
        "    for i, (resp, item) in enumerate(zip(model_responses, test_data)):\n",
        "        comet_data.append({\"src\": item['query'], \"mt\": resp, \"ref\": item['reference']})\n",
        "\n",
        "    # Run COMET evaluation\n",
        "    print(\"Running COMET evaluation (deterministic)...\")\n",
        "    try:\n",
        "        comet_output = comet_model.predict(comet_data, batch_size=8)\n",
        "        if isinstance(comet_output, dict) and 'scores' in comet_output:\n",
        "            comet_scores = comet_output['scores']\n",
        "        elif isinstance(comet_output, list):\n",
        "            comet_scores = comet_output\n",
        "        else:\n",
        "            comet_scores = [float(s) if isinstance(s, (int, float)) else 0.0 for s in comet_output]\n",
        "\n",
        "        valid_scores = [float(s) for s in comet_scores]\n",
        "        avg_score = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0\n",
        "        min_score = min(valid_scores) if valid_scores else 0.0\n",
        "        max_score = max(valid_scores) if valid_scores else 0.0\n",
        "\n",
        "        # Length stats\n",
        "        avg_len = (sum(lengths) / len(lengths)) if lengths else 0.0\n",
        "        min_len = min(lengths) if lengths else 0\n",
        "        max_len = max(lengths) if lengths else 0\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"DETERMINISTIC COMET EVALUATION (Two-Stage, place-normalized)\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Total Queries: {len(comet_data)}\")\n",
        "        print(f\"Average COMET: {avg_score:.4f}  |  Min: {min_score:.4f}  |  Max: {max_score:.4f}\")\n",
        "        print(f\"Answer length (words): avg={avg_len:.1f}, min={min_len}, max={max_len}\")\n",
        "\n",
        "        print(\"\\nINDIVIDUAL RESULTS\")\n",
        "        for i, (item, score, resp) in enumerate(zip(test_data, valid_scores, model_responses), 1):\n",
        "            print(f\"{i:02d}. {item['query']}  |  Score={score:.4f}  |  len={len(resp.split())}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"COMET evaluation error: {e}\")\n",
        "        comet_scores = [0.0 for _ in comet_data]\n",
        "        avg_score = 0.0\n",
        "        min_score = 0.0\n",
        "        max_score = 0.0\n",
        "\n",
        "    # Save detailed results\n",
        "    results = {\n",
        "        \"evaluation_metadata\": {\n",
        "            \"total_queries\": len(comet_data),\n",
        "            \"average_score\": avg_score,\n",
        "            \"min_score\": min_score,\n",
        "            \"max_score\": max_score,\n",
        "            \"evaluation_date\": str(pd.Timestamp.now()),\n",
        "            \"place_normalization_applied\": True,\n",
        "            \"deterministic\": True,\n",
        "            \"num_beams\": 4,\n",
        "            \"repetition_penalty\": 1.05,\n",
        "        },\n",
        "        \"test_data\": test_data,\n",
        "        \"model_responses\": model_responses,\n",
        "        \"comet_scores\": comet_scores,\n",
        "        \"length_words\": lengths,\n",
        "    }\n",
        "\n",
        "    with open(str(OUTPUT_DIR / \"comet_evaluation_deterministic.json\"), \"w\" ) as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    import pandas as pd\n",
        "    pd.DataFrame({\n",
        "        'query': [t['query'] for t in test_data],\n",
        "        'model_response': model_responses,\n",
        "        'reference': [t['reference'] for t in test_data],\n",
        "        'comet_score': comet_scores,\n",
        "        'answer_len_words': lengths,\n",
        "    }).to_csv(str(OUTPUT_DIR / \"comet_evaluation_deterministic.csv\"), index=False)\n",
        "    print(f\"\\n📁 Deterministic results saved to: {OUTPUT_DIR / 'comet_evaluation_deterministic.json'}\")\n",
        "    print(f\"📊 CSV saved to: {OUTPUT_DIR / 'comet_evaluation_deterministic.csv'}\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"COMET not available. Install with: pip install unbabel-comet\")\n",
        "except Exception as e:\n",
        "    print(f\"COMET evaluation failed: {e}\")\n",
        "    print(\"Make sure you have a trained model and COMET installed.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
